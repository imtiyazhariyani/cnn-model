{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#KEVIN VERSION 1 - Preliminary code used for group presentation and training the first model\n",
        "#This is the original code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Create a ColumnTransformer to transform categorical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the ColumnTransformer on the DataFrame\n",
        "df_transformed = preprocessor.fit_transform(df)\n",
        "\n",
        "# Update feature names after one-hot encoding (for older scikit-learn versions)\n",
        "feature_names = list(numeric_cols)\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].dropna().unique()\n",
        "    feature_names.extend([f'{col}_{val}' for val in unique_values])\n",
        "\n",
        "# Convert the transformed array back to a DataFrame\n",
        "df_transformed = pd.DataFrame(df_transformed, columns=feature_names)\n",
        "\n",
        "# Specify gene expression columns\n",
        "gene_expressions = ['ENSG00000146938_Expression', 'ENSG00000101849_Expression', 'ENSG00000047644_Expression', 'ENSG00000073464_Expression', 'ENSG00000101871_Expression']\n",
        "\n",
        "# Select features and targets\n",
        "feature_columns = df_transformed.columns.difference(gene_expressions)\n",
        "X = df_transformed[feature_columns]\n",
        "y = df_transformed[gene_expressions]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for CNN\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(5))  # Output layer for 5 gene expressions\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Model Loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwu9xb8wO67y",
        "outputId": "4dd6be39-32ff-49a5-e0ac-d40169b30b30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "137/137 [==============================] - 3s 9ms/step - loss: 0.1350 - val_loss: 0.1321\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1317 - val_loss: 0.1310\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1313 - val_loss: 0.1317\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1309 - val_loss: 0.1306\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1302 - val_loss: 0.1308\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1301 - val_loss: 0.1295\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1297 - val_loss: 0.1320\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1298 - val_loss: 0.1304\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1297 - val_loss: 0.1308\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1296 - val_loss: 0.1308\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1294 - val_loss: 0.1304\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1294 - val_loss: 0.1307\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1293 - val_loss: 0.1300\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1292 - val_loss: 0.1306\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1291 - val_loss: 0.1321\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1291 - val_loss: 0.1324\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1288 - val_loss: 0.1305\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1289 - val_loss: 0.1311\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1290 - val_loss: 0.1303\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1288 - val_loss: 0.1308\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1288 - val_loss: 0.1307\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1287 - val_loss: 0.1310\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 1s 9ms/step - loss: 0.1286 - val_loss: 0.1313\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.1310\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1284 - val_loss: 0.1314\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1283 - val_loss: 0.1306\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1283 - val_loss: 0.1309\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1282 - val_loss: 0.1309\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1283 - val_loss: 0.1305\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1282 - val_loss: 0.1318\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1280 - val_loss: 0.1306\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1278 - val_loss: 0.1311\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1279 - val_loss: 0.1310\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1277 - val_loss: 0.1318\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1278 - val_loss: 0.1313\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1275 - val_loss: 0.1314\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1275 - val_loss: 0.1315\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1273 - val_loss: 0.1315\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1274 - val_loss: 0.1316\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1275 - val_loss: 0.1313\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1273 - val_loss: 0.1313\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1273 - val_loss: 0.1323\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1271 - val_loss: 0.1317\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1271 - val_loss: 0.1316\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1270 - val_loss: 0.1321\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.1318\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1270 - val_loss: 0.1317\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1269 - val_loss: 0.1328\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.1322\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1268 - val_loss: 0.1324\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1266 - val_loss: 0.1320\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1266 - val_loss: 0.1328\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1265 - val_loss: 0.1326\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1265 - val_loss: 0.1322\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1265 - val_loss: 0.1327\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1266 - val_loss: 0.1328\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1263 - val_loss: 0.1326\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1265 - val_loss: 0.1328\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1262 - val_loss: 0.1335\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1263 - val_loss: 0.1330\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1262 - val_loss: 0.1329\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1261 - val_loss: 0.1330\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1264 - val_loss: 0.1328\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1261 - val_loss: 0.1330\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1260 - val_loss: 0.1325\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1260 - val_loss: 0.1331\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1261 - val_loss: 0.1330\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1259 - val_loss: 0.1332\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1258 - val_loss: 0.1337\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1257 - val_loss: 0.1337\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1257 - val_loss: 0.1335\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1258 - val_loss: 0.1342\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.1337\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1255 - val_loss: 0.1335\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1255 - val_loss: 0.1334\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1255 - val_loss: 0.1340\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1337\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1338\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1347\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1252 - val_loss: 0.1341\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1254 - val_loss: 0.1338\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1339\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1250 - val_loss: 0.1342\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1346\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1250 - val_loss: 0.1345\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1249 - val_loss: 0.1348\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1248 - val_loss: 0.1351\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.1341\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1249 - val_loss: 0.1347\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1247 - val_loss: 0.1347\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.1350\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1249 - val_loss: 0.1350\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.1348\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.1347\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1246 - val_loss: 0.1356\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1247 - val_loss: 0.1352\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1246 - val_loss: 0.1347\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1246 - val_loss: 0.1348\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1245 - val_loss: 0.1353\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1243 - val_loss: 0.1355\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1355\n",
            "Model Loss: 0.13548335433006287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KEVIN VERSION 2 - Code with Class_weight\n",
        "#Code with class_weight\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Create a ColumnTransformer to transform categorical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the ColumnTransformer on the DataFrame\n",
        "df_transformed = preprocessor.fit_transform(df)\n",
        "\n",
        "# Update feature names after one-hot encoding\n",
        "feature_names = list(numeric_cols)\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].dropna().unique()\n",
        "    feature_names.extend([f'{col}_{val}' for val in unique_values])\n",
        "\n",
        "# Convert the transformed array back to a DataFrame\n",
        "df_transformed = pd.DataFrame(df_transformed, columns=feature_names)\n",
        "\n",
        "# Specify gene expression columns\n",
        "gene_expressions = ['ENSG00000146938_Expression', 'ENSG00000101849_Expression', 'ENSG00000047644_Expression', 'ENSG00000073464_Expression', 'ENSG00000101871_Expression']\n",
        "\n",
        "# Select features and targets\n",
        "feature_columns = df_transformed.columns.difference(gene_expressions)\n",
        "X = df_transformed[feature_columns]\n",
        "y = df_transformed[gene_expressions]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for CNN\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Class weights (adjust as needed based on your dataset)\n",
        "class_weights = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1}\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(5, kernel_regularizer=l2(0.01)))  # Output layer for 5 gene expressions\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, class_weight=class_weights, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Model Loss: {loss}')\n"
      ],
      "metadata": {
        "id": "hNOTqdqVO-Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29477341-7318-48e8-beb6-e73c2ec94ce5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.4132 - val_loss: 0.1764\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1566 - val_loss: 0.1454\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1408 - val_loss: 0.1385\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1368 - val_loss: 0.1357\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1349 - val_loss: 0.1347\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1341 - val_loss: 0.1348\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1338 - val_loss: 0.1339\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1335 - val_loss: 0.1333\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1332 - val_loss: 0.1336\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1332 - val_loss: 0.1332\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1330 - val_loss: 0.1332\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1329 - val_loss: 0.1332\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1327 - val_loss: 0.1328\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1325\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1325\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1325 - val_loss: 0.1327\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1325 - val_loss: 0.1324\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1326\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1326\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1327\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1325\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1325\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1326\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1326\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1327\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1323\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1326\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1326\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1325\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.1323\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1325\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1325\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1325\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1325\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.1323\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1323\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1325\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1322 - val_loss: 0.1324\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1321 - val_loss: 0.1324\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1324\n",
            "Model Loss: 0.1323697715997696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KEVIN VERSION 3\n",
        "# it seems that there might be an imbalance in the 'age_group' category\n",
        "#This code modification will ensure that the proportions of different age groups are similar in both the training and test sets.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Create a ColumnTransformer to transform categorical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the ColumnTransformer on the DataFrame\n",
        "df_transformed = preprocessor.fit_transform(df)\n",
        "\n",
        "# Update feature names after one-hot encoding\n",
        "feature_names = list(numeric_cols)\n",
        "for col in categorical_cols:\n",
        "    unique_values = df[col].dropna().unique()\n",
        "    feature_names.extend([f'{col}_{val}' for val in unique_values])\n",
        "\n",
        "# Convert the transformed array back to a DataFrame\n",
        "df_transformed = pd.DataFrame(df_transformed, columns=feature_names)\n",
        "\n",
        "# Specify gene expression columns\n",
        "gene_expressions = ['ENSG00000146938_Expression', 'ENSG00000101849_Expression', 'ENSG00000047644_Expression', 'ENSG00000073464_Expression', 'ENSG00000101871_Expression']\n",
        "\n",
        "# Select features and targets\n",
        "feature_columns = df_transformed.columns.difference(gene_expressions)\n",
        "X = df_transformed[feature_columns]\n",
        "y = df_transformed[gene_expressions]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Stratified train-test split based on 'age_group'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=df['age_group'])\n",
        "\n",
        "# Reshape input data for CNN\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(len(gene_expressions), kernel_regularizer=l2(0.01)))  # Output layer for gene expressions\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Model Loss: {loss}')\n"
      ],
      "metadata": {
        "id": "o4UiD4zcrp1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a957e1a-ee87-4fa6-e3bc-6ad3527838a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "137/137 [==============================] - 2s 8ms/step - loss: 0.4129 - val_loss: 0.1776\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1576 - val_loss: 0.1448\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1413 - val_loss: 0.1375\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1370 - val_loss: 0.1357\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1354 - val_loss: 0.1341\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1349 - val_loss: 0.1343\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1342 - val_loss: 0.1330\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1341 - val_loss: 0.1329\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1335 - val_loss: 0.1326\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1333 - val_loss: 0.1324\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1333 - val_loss: 0.1324\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1330 - val_loss: 0.1327\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1332 - val_loss: 0.1321\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1329 - val_loss: 0.1322\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1327 - val_loss: 0.1321\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1327 - val_loss: 0.1319\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1327 - val_loss: 0.1318\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1326 - val_loss: 0.1317\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1325 - val_loss: 0.1319\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1326 - val_loss: 0.1317\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1325 - val_loss: 0.1316\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1325 - val_loss: 0.1318\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1318\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1319\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1325 - val_loss: 0.1316\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1325 - val_loss: 0.1315\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1316\n",
            "Model Loss: 0.13155841827392578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qQgFKcYI7k3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4480365b-526f-4d31-b9ad-d621099b1b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "137/137 [==============================] - 2s 10ms/step - loss: 0.3866 - val_loss: 0.1712\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1551 - val_loss: 0.1452\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1411 - val_loss: 0.1372\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1369 - val_loss: 0.1359\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1353 - val_loss: 0.1342\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1346 - val_loss: 0.1334\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1342 - val_loss: 0.1331\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1340 - val_loss: 0.1330\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1338 - val_loss: 0.1327\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1335 - val_loss: 0.1323\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1333 - val_loss: 0.1342\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1332 - val_loss: 0.1325\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1330 - val_loss: 0.1321\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1329 - val_loss: 0.1320\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1328 - val_loss: 0.1322\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1327 - val_loss: 0.1320\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1327 - val_loss: 0.1320\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1326 - val_loss: 0.1320\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1317\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1325 - val_loss: 0.1318\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1317\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1325 - val_loss: 0.1316\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1325 - val_loss: 0.1316\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1325 - val_loss: 0.1316\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1318\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1318\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1317\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 1s 6ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1323 - val_loss: 0.1315\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1315\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1324 - val_loss: 0.1316\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1316\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 0.1316\n",
            "Model Loss: 0.13164016604423523\n"
          ]
        }
      ],
      "source": [
        "#KEVIN VERSION 4 - Included Start & Stop coordinates for genes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Identify all genes by looking for unique identifiers in the column names\n",
        "genes = set(col.split('_')[0] for col in df.columns if '_Start' in col or '_Stop' in col or '_End' in col)\n",
        "\n",
        "# For each gene, calculate the length as Stop - Start\n",
        "for gene in genes:\n",
        "    start_col = f'{gene}_Start'\n",
        "    stop_col = f'{gene}_Stop' if f'{gene}_Stop' in df.columns else f'{gene}_End'\n",
        "\n",
        "    # Ensure the start and stop columns are numeric\n",
        "    df[start_col] = pd.to_numeric(df[start_col], errors='coerce')\n",
        "    df[stop_col] = pd.to_numeric(df[stop_col], errors='coerce')\n",
        "\n",
        "    # Create a new column for gene length\n",
        "    length_col = f'{gene}_Length'\n",
        "    df[length_col] = df[stop_col] - df[start_col]\n",
        "\n",
        "# Fill missing values in specific columns with a default value (e.g., 0)\n",
        "default_value = 0\n",
        "df.fillna(default_value, inplace=True)\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.union([f'{gene}_Length' for gene in genes])\n",
        "\n",
        "# Create a ColumnTransformer to transform categorical columns and scale numeric columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), list(numeric_cols)),\n",
        "        ('cat', OneHotEncoder(), list(categorical_cols))\n",
        "    ])\n",
        "\n",
        "# Fit and transform the ColumnTransformer on the DataFrame\n",
        "df_transformed = preprocessor.fit_transform(df)\n",
        "\n",
        "# Collect feature names from the ColumnTransformer\n",
        "numeric_features = preprocessor.named_transformers_['num'].get_feature_names_out()\n",
        "categorical_features = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
        "\n",
        "# Combine all feature names (numeric and categorical)\n",
        "all_feature_names = list(numeric_features) + list(categorical_features)\n",
        "\n",
        "# Convert the transformed array back to a DataFrame\n",
        "df_transformed = pd.DataFrame(df_transformed, columns=all_feature_names)\n",
        "\n",
        "# Specify gene expression columns\n",
        "gene_expressions = ['ENSG00000146938_Expression', 'ENSG00000101849_Expression', 'ENSG00000047644_Expression', 'ENSG00000073464_Expression', 'ENSG00000101871_Expression']\n",
        "\n",
        "# Select features and targets\n",
        "feature_columns = df_transformed.columns.difference(gene_expressions)\n",
        "X = df_transformed[feature_columns]\n",
        "y = df_transformed[gene_expressions]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=df['age_group'])\n",
        "\n",
        "# Reshape input data for CNN\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(len(gene_expressions), kernel_regularizer=l2(0.01)))  # Output layer for gene expressions\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Model Loss: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 1 - Fixed issues with input features including chromatin accessibility which was not included and gene coordinates; removed scaling;\n",
        "# fixed CNN input dimensions, ran the model on test dataset for 5 genes, 1000 epochs\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        # Calculate the size of the linear layer input based on the output size after convolutions\n",
        "        # conv_output_size = 64 * (((input_size - 2) // 2 - 2) // 2 - 2)\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 5)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        a = a.squeeze(dim=-1)\n",
        "\n",
        "        #print(x.size(-2),x.size(-1),x.size(0),x.size(1))\n",
        "        #print(a.size(-2),a.size(-1),a.size(0),a.size(1))\n",
        "\n",
        "        # Ensure that the size of dimension 1 is consistent\n",
        "\n",
        "\n",
        "        x = torch.cat([x, a], dim=1)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x.view(x.size(0),-1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.model = None\n",
        "        self.input_size = None\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Identify gene expression columns\n",
        "        gene_expression_columns = self.df.filter(like='_Expression').columns\n",
        "\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features) & set(self.df.columns))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        default_value = 0\n",
        "        self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = chromatin_accessibility_columns.union(additional_categorical_features)\n",
        "\n",
        "        # Select features and target features\n",
        "        X = self.df[feature_columns].copy()\n",
        "        y = self.df[gene_expression_columns].copy()\n",
        "\n",
        "        # Convert categorical columns to one-hot encoding\n",
        "        X = pd.get_dummies(X, columns=additional_categorical_features)\n",
        "\n",
        "        # Train-validation-test split\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Reshape input data for CNN\n",
        "        self.X_train_tensor = torch.tensor(X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_val_tensor = torch.tensor(X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "        print(\"Shapes after reshaping:\")\n",
        "        print(\"X_train_tensor:\", self.X_train_tensor.shape)\n",
        "        print(\"y_train_tensor:\", self.y_train_tensor.shape)\n",
        "        print(\"X_val_tensor:\", self.X_val_tensor.shape)\n",
        "        print(\"y_val_tensor:\", self.y_val_tensor.shape)\n",
        "        print(\"X_test_tensor:\", self.X_test_tensor.shape)\n",
        "        print(\"y_test_tensor:\", self.y_test_tensor.shape)\n",
        "\n",
        "        self.input_size = X.shape[1]\n",
        "\n",
        "\n",
        "    def train_model(self, hidden_size=5, dropout_rate=0.2, epochs=1000, batch_size=32):\n",
        "        model = CNNModel(input_size=self.input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Store the training and validation loss for plotting\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(self.X_train_tensor, self.X_train_tensor[:, -1:, :])\n",
        "            loss = criterion(outputs, self.y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(self.X_val_tensor, self.X_val_tensor[:, -1:, :])\n",
        "                val_loss = criterion(val_outputs, self.y_val_tensor)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "        # Plot the training and validation loss\n",
        "        plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        if self.model is None:\n",
        "            print(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return\n",
        "\n",
        "        model = self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(self.X_test_tensor, self.X_test_tensor[:, -1:, :])\n",
        "            test_loss = criterion(test_outputs, self.y_test_tensor)\n",
        "\n",
        "        print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "gene_predictor.train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "id": "VFaZeG3p7qi0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4187222e-2ee5-4a8b-b84b-78a40ce2907c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes after reshaping:\n",
            "X_train_tensor: torch.Size([3275, 11, 1])\n",
            "y_train_tensor: torch.Size([3275, 5])\n",
            "X_val_tensor: torch.Size([1092, 11, 1])\n",
            "y_val_tensor: torch.Size([1092, 5])\n",
            "X_test_tensor: torch.Size([1092, 11, 1])\n",
            "y_test_tensor: torch.Size([1092, 5])\n",
            "Epoch 10/1000, Training Loss: 0.38426119089126587, Validation Loss: 0.23461835086345673\n",
            "Epoch 20/1000, Training Loss: 0.3312271237373352, Validation Loss: 0.23835992813110352\n",
            "Epoch 30/1000, Training Loss: 0.2986294627189636, Validation Loss: 0.2394130378961563\n",
            "Epoch 40/1000, Training Loss: 0.2741817533969879, Validation Loss: 0.23572707176208496\n",
            "Epoch 50/1000, Training Loss: 0.26096948981285095, Validation Loss: 0.22850297391414642\n",
            "Epoch 60/1000, Training Loss: 0.24862271547317505, Validation Loss: 0.22128622233867645\n",
            "Epoch 70/1000, Training Loss: 0.23605915904045105, Validation Loss: 0.21441878378391266\n",
            "Epoch 80/1000, Training Loss: 0.22766461968421936, Validation Loss: 0.20750214159488678\n",
            "Epoch 90/1000, Training Loss: 0.21689867973327637, Validation Loss: 0.20072898268699646\n",
            "Epoch 100/1000, Training Loss: 0.21023282408714294, Validation Loss: 0.19453109800815582\n",
            "Epoch 110/1000, Training Loss: 0.20613130927085876, Validation Loss: 0.1888546198606491\n",
            "Epoch 120/1000, Training Loss: 0.19885636866092682, Validation Loss: 0.18362130224704742\n",
            "Epoch 130/1000, Training Loss: 0.1959843784570694, Validation Loss: 0.1786186695098877\n",
            "Epoch 140/1000, Training Loss: 0.19047483801841736, Validation Loss: 0.17426635324954987\n",
            "Epoch 150/1000, Training Loss: 0.18472200632095337, Validation Loss: 0.1701015830039978\n",
            "Epoch 160/1000, Training Loss: 0.18181735277175903, Validation Loss: 0.1662331521511078\n",
            "Epoch 170/1000, Training Loss: 0.17751549184322357, Validation Loss: 0.16267552971839905\n",
            "Epoch 180/1000, Training Loss: 0.17463964223861694, Validation Loss: 0.1594196856021881\n",
            "Epoch 190/1000, Training Loss: 0.17078959941864014, Validation Loss: 0.15651075541973114\n",
            "Epoch 200/1000, Training Loss: 0.16674889624118805, Validation Loss: 0.15399084985256195\n",
            "Epoch 210/1000, Training Loss: 0.1645122468471527, Validation Loss: 0.1514061987400055\n",
            "Epoch 220/1000, Training Loss: 0.1597563475370407, Validation Loss: 0.1492379754781723\n",
            "Epoch 230/1000, Training Loss: 0.15930050611495972, Validation Loss: 0.14729538559913635\n",
            "Epoch 240/1000, Training Loss: 0.1578386425971985, Validation Loss: 0.1459391713142395\n",
            "Epoch 250/1000, Training Loss: 0.1540161520242691, Validation Loss: 0.14449089765548706\n",
            "Epoch 260/1000, Training Loss: 0.1514672338962555, Validation Loss: 0.14305555820465088\n",
            "Epoch 270/1000, Training Loss: 0.15240709483623505, Validation Loss: 0.1420612782239914\n",
            "Epoch 280/1000, Training Loss: 0.14897920191287994, Validation Loss: 0.14113174378871918\n",
            "Epoch 290/1000, Training Loss: 0.1481029987335205, Validation Loss: 0.14038468897342682\n",
            "Epoch 300/1000, Training Loss: 0.14584249258041382, Validation Loss: 0.13968633115291595\n",
            "Epoch 310/1000, Training Loss: 0.14601434767246246, Validation Loss: 0.1390855610370636\n",
            "Epoch 320/1000, Training Loss: 0.14515632390975952, Validation Loss: 0.13853730261325836\n",
            "Epoch 330/1000, Training Loss: 0.143210768699646, Validation Loss: 0.13813108205795288\n",
            "Epoch 340/1000, Training Loss: 0.1423700451850891, Validation Loss: 0.1377994865179062\n",
            "Epoch 350/1000, Training Loss: 0.1423403024673462, Validation Loss: 0.13743793964385986\n",
            "Epoch 360/1000, Training Loss: 0.14192211627960205, Validation Loss: 0.13707847893238068\n",
            "Epoch 370/1000, Training Loss: 0.13956402242183685, Validation Loss: 0.13698457181453705\n",
            "Epoch 380/1000, Training Loss: 0.14013582468032837, Validation Loss: 0.13688670098781586\n",
            "Epoch 390/1000, Training Loss: 0.13904935121536255, Validation Loss: 0.1366906315088272\n",
            "Epoch 400/1000, Training Loss: 0.13857997953891754, Validation Loss: 0.13651829957962036\n",
            "Epoch 410/1000, Training Loss: 0.13843071460723877, Validation Loss: 0.13632158935070038\n",
            "Epoch 420/1000, Training Loss: 0.13723637163639069, Validation Loss: 0.13617822527885437\n",
            "Epoch 430/1000, Training Loss: 0.13711704313755035, Validation Loss: 0.1361122727394104\n",
            "Epoch 440/1000, Training Loss: 0.13686203956604004, Validation Loss: 0.1360822468996048\n",
            "Epoch 450/1000, Training Loss: 0.13652294874191284, Validation Loss: 0.13609261810779572\n",
            "Epoch 460/1000, Training Loss: 0.13483037054538727, Validation Loss: 0.13597247004508972\n",
            "Epoch 470/1000, Training Loss: 0.1351872980594635, Validation Loss: 0.13584811985492706\n",
            "Epoch 480/1000, Training Loss: 0.1365528702735901, Validation Loss: 0.135828897356987\n",
            "Epoch 490/1000, Training Loss: 0.1356613039970398, Validation Loss: 0.13577529788017273\n",
            "Epoch 500/1000, Training Loss: 0.13486503064632416, Validation Loss: 0.1357526034116745\n",
            "Epoch 510/1000, Training Loss: 0.1342501938343048, Validation Loss: 0.1357613056898117\n",
            "Epoch 520/1000, Training Loss: 0.13348306715488434, Validation Loss: 0.13565054535865784\n",
            "Epoch 530/1000, Training Loss: 0.13377368450164795, Validation Loss: 0.13556990027427673\n",
            "Epoch 540/1000, Training Loss: 0.13383449614048004, Validation Loss: 0.1354806125164032\n",
            "Epoch 550/1000, Training Loss: 0.1332092434167862, Validation Loss: 0.13545502722263336\n",
            "Epoch 560/1000, Training Loss: 0.13336510956287384, Validation Loss: 0.13543038070201874\n",
            "Epoch 570/1000, Training Loss: 0.1326068639755249, Validation Loss: 0.13541050255298615\n",
            "Epoch 580/1000, Training Loss: 0.13347303867340088, Validation Loss: 0.13539643585681915\n",
            "Epoch 590/1000, Training Loss: 0.1314195692539215, Validation Loss: 0.13536426424980164\n",
            "Epoch 600/1000, Training Loss: 0.1321374475955963, Validation Loss: 0.13535790145397186\n",
            "Epoch 610/1000, Training Loss: 0.13171622157096863, Validation Loss: 0.13538874685764313\n",
            "Epoch 620/1000, Training Loss: 0.1313960701227188, Validation Loss: 0.1353597491979599\n",
            "Epoch 630/1000, Training Loss: 0.1316048949956894, Validation Loss: 0.13531532883644104\n",
            "Epoch 640/1000, Training Loss: 0.13142600655555725, Validation Loss: 0.1353168934583664\n",
            "Epoch 650/1000, Training Loss: 0.13142727315425873, Validation Loss: 0.13532666862010956\n",
            "Epoch 660/1000, Training Loss: 0.1303878128528595, Validation Loss: 0.13530433177947998\n",
            "Epoch 670/1000, Training Loss: 0.13033601641654968, Validation Loss: 0.1352873295545578\n",
            "Epoch 680/1000, Training Loss: 0.13109621405601501, Validation Loss: 0.13528990745544434\n",
            "Epoch 690/1000, Training Loss: 0.1297992765903473, Validation Loss: 0.13530629873275757\n",
            "Epoch 700/1000, Training Loss: 0.13038083910942078, Validation Loss: 0.13526597619056702\n",
            "Epoch 710/1000, Training Loss: 0.13025693595409393, Validation Loss: 0.13521984219551086\n",
            "Epoch 720/1000, Training Loss: 0.13040988147258759, Validation Loss: 0.1352495551109314\n",
            "Epoch 730/1000, Training Loss: 0.13013534247875214, Validation Loss: 0.13528646528720856\n",
            "Epoch 740/1000, Training Loss: 0.1301199197769165, Validation Loss: 0.13526688516139984\n",
            "Epoch 750/1000, Training Loss: 0.12990440428256989, Validation Loss: 0.13528390228748322\n",
            "Epoch 760/1000, Training Loss: 0.1303820013999939, Validation Loss: 0.13530929386615753\n",
            "Epoch 770/1000, Training Loss: 0.1292961835861206, Validation Loss: 0.13528980314731598\n",
            "Epoch 780/1000, Training Loss: 0.12972629070281982, Validation Loss: 0.1352825164794922\n",
            "Epoch 790/1000, Training Loss: 0.12906891107559204, Validation Loss: 0.13524551689624786\n",
            "Epoch 800/1000, Training Loss: 0.1291908174753189, Validation Loss: 0.13521628081798553\n",
            "Epoch 810/1000, Training Loss: 0.12871526181697845, Validation Loss: 0.1352786272764206\n",
            "Epoch 820/1000, Training Loss: 0.12932486832141876, Validation Loss: 0.1353474110364914\n",
            "Epoch 830/1000, Training Loss: 0.12904193997383118, Validation Loss: 0.1353013962507248\n",
            "Epoch 840/1000, Training Loss: 0.12826278805732727, Validation Loss: 0.13521842658519745\n",
            "Epoch 850/1000, Training Loss: 0.129116952419281, Validation Loss: 0.1352059245109558\n",
            "Epoch 860/1000, Training Loss: 0.1287144124507904, Validation Loss: 0.13520777225494385\n",
            "Epoch 870/1000, Training Loss: 0.12841296195983887, Validation Loss: 0.13527609407901764\n",
            "Epoch 880/1000, Training Loss: 0.12883038818836212, Validation Loss: 0.1353226602077484\n",
            "Epoch 890/1000, Training Loss: 0.1288318783044815, Validation Loss: 0.13532139360904694\n",
            "Epoch 900/1000, Training Loss: 0.12844008207321167, Validation Loss: 0.13528308272361755\n",
            "Epoch 910/1000, Training Loss: 0.12863904237747192, Validation Loss: 0.13523533940315247\n",
            "Epoch 920/1000, Training Loss: 0.12855178117752075, Validation Loss: 0.13522306084632874\n",
            "Epoch 930/1000, Training Loss: 0.12799443304538727, Validation Loss: 0.1352630853652954\n",
            "Epoch 940/1000, Training Loss: 0.12848618626594543, Validation Loss: 0.13527269661426544\n",
            "Epoch 950/1000, Training Loss: 0.12780913710594177, Validation Loss: 0.1352720558643341\n",
            "Epoch 960/1000, Training Loss: 0.1274690479040146, Validation Loss: 0.13529300689697266\n",
            "Epoch 970/1000, Training Loss: 0.12815874814987183, Validation Loss: 0.1352730095386505\n",
            "Epoch 980/1000, Training Loss: 0.12755462527275085, Validation Loss: 0.13526730239391327\n",
            "Epoch 990/1000, Training Loss: 0.1278410404920578, Validation Loss: 0.13518427312374115\n",
            "Epoch 1000/1000, Training Loss: 0.12803047895431519, Validation Loss: 0.1351201832294464\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.13110452890396118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 2 - Updated code to work with paired data (RNA+ATAC) rather than pools of data. This code creates one model per gene to predict gene expression from chromatin accessibility data\n",
        " #alongside other features. Still not ideal because we want a single model to be able to learn from other genes but at least now there is some specificity with RNA-ATAC pairs.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PairedCNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2, num_genes=5):\n",
        "        super(PairedCNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 5)\n",
        "\n",
        "        # Separate branches for each gene\n",
        "        self.gene_branches = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(64, hidden_size),\n",
        "                nn.BatchNorm1d(hidden_size),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_size, 1)\n",
        "            )\n",
        "            for _ in range(num_genes)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Separate fully connected layer\n",
        "        x_fc = F.relu(self.fc1(x))\n",
        "        x_fc = self.batch_norm3(x_fc)\n",
        "        x_fc = self.dropout3(x_fc)\n",
        "\n",
        "        # Separate branches for each gene\n",
        "        gene_predictions = [gene_branch(x) for gene_branch in self.gene_branches]\n",
        "\n",
        "        # Concatenate the gene-specific predictions\n",
        "        x = torch.cat(gene_predictions, dim=1)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.model = None\n",
        "        self.input_size = None\n",
        "        self.gene_data = {}  # Dictionary to store data for each gene\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Identify gene expression columns\n",
        "        gene_expression_columns = self.df.filter(like='_Expression').columns\n",
        "\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features) & set(self.df.columns))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        default_value = 0\n",
        "        self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = chromatin_accessibility_columns.union(additional_categorical_features)\n",
        "\n",
        "        # Iterate over each gene to create subsets\n",
        "        for gene in gene_expression_columns:\n",
        "            # Select features and target features for the current gene\n",
        "            X_gene = self.df[feature_columns].copy()\n",
        "            y_gene = self.df[gene].copy()\n",
        "            #print(y_gene)\n",
        "\n",
        "            # Convert categorical columns to one-hot encoding\n",
        "            X_gene = pd.get_dummies(X_gene, columns=additional_categorical_features)\n",
        "\n",
        "            # Reshape input data for CNN\n",
        "            X_gene_tensor = torch.tensor(X_gene.values.reshape(X_gene.shape[0], X_gene.shape[1], 1), dtype=torch.float32)\n",
        "            y_gene_tensor = torch.tensor(y_gene.values, dtype=torch.float32)\n",
        "\n",
        "            # Store gene-specific data in the dictionary\n",
        "            self.gene_data[gene] = {'X_tensor': X_gene_tensor, 'y_tensor': y_gene_tensor}\n",
        "\n",
        "            print(f\"Shapes after reshaping for {gene}:\")\n",
        "            print(\"X_tensor:\", X_gene_tensor.shape)\n",
        "            print(\"y_tensor:\", y_gene_tensor.shape)\n",
        "\n",
        "        self.input_size = X_gene.shape[1]\n",
        "\n",
        "\n",
        "    def train_model(self, hidden_size=5, dropout_rate=0.2, epochs=100, batch_size=32):\n",
        "        for gene, data in self.gene_data.items():\n",
        "            X_gene_tensor = data['X_tensor']\n",
        "            y_gene_tensor = data['y_tensor']\n",
        "\n",
        "            model = PairedCNNModel(input_size=self.input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "            # Store the training and validation loss for plotting\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                # Training\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_gene_tensor, X_gene_tensor[:, -1:, -1:])\n",
        "                loss = criterion(outputs, y_gene_tensor.unsqueeze(1).expand_as(outputs))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                # Validation (you can add a validation set if needed)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_gene_tensor, X_gene_tensor[:, -1:, :])\n",
        "                    val_loss = criterion(val_outputs, y_gene_tensor.unsqueeze(1).expand_as(val_outputs))\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f'Gene: {gene}, Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "            # Plot the training and validation loss for each gene\n",
        "            plt.plot(range(1, epochs+1), train_losses, label=f'Training Loss ({gene})')\n",
        "            plt.plot(range(1, epochs+1), val_losses, label=f'Validation Loss ({gene})')\n",
        "\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "gene_predictor.train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "#gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "id": "bHGV8J4gJ6F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfd27d8-33ed-4d1d-ada6-c7e9f9792ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes after reshaping for ENSG00000146938_Expression:\n",
            "X_tensor: torch.Size([5459, 11, 1])\n",
            "y_tensor: torch.Size([5459])\n",
            "Shapes after reshaping for ENSG00000101849_Expression:\n",
            "X_tensor: torch.Size([5459, 11, 1])\n",
            "y_tensor: torch.Size([5459])\n",
            "Shapes after reshaping for ENSG00000047644_Expression:\n",
            "X_tensor: torch.Size([5459, 11, 1])\n",
            "y_tensor: torch.Size([5459])\n",
            "Shapes after reshaping for ENSG00000073464_Expression:\n",
            "X_tensor: torch.Size([5459, 11, 1])\n",
            "y_tensor: torch.Size([5459])\n",
            "Shapes after reshaping for ENSG00000101871_Expression:\n",
            "X_tensor: torch.Size([5459, 11, 1])\n",
            "y_tensor: torch.Size([5459])\n",
            "Gene: ENSG00000146938_Expression, Epoch 10/100, Training Loss: 0.6187929511070251, Validation Loss: 0.4242611825466156\n",
            "Gene: ENSG00000146938_Expression, Epoch 20/100, Training Loss: 0.5490797758102417, Validation Loss: 0.41789546608924866\n",
            "Gene: ENSG00000146938_Expression, Epoch 30/100, Training Loss: 0.507340669631958, Validation Loss: 0.4045029282569885\n",
            "Gene: ENSG00000146938_Expression, Epoch 40/100, Training Loss: 0.4852050542831421, Validation Loss: 0.3974761366844177\n",
            "Gene: ENSG00000146938_Expression, Epoch 50/100, Training Loss: 0.4662999212741852, Validation Loss: 0.38850313425064087\n",
            "Gene: ENSG00000146938_Expression, Epoch 60/100, Training Loss: 0.44731903076171875, Validation Loss: 0.376810759305954\n",
            "Gene: ENSG00000146938_Expression, Epoch 70/100, Training Loss: 0.4288671910762787, Validation Loss: 0.3654927909374237\n",
            "Gene: ENSG00000146938_Expression, Epoch 80/100, Training Loss: 0.4203856885433197, Validation Loss: 0.3555331528186798\n",
            "Gene: ENSG00000146938_Expression, Epoch 90/100, Training Loss: 0.407990425825119, Validation Loss: 0.34559378027915955\n",
            "Gene: ENSG00000146938_Expression, Epoch 100/100, Training Loss: 0.3912413418292999, Validation Loss: 0.3379773199558258\n",
            "Gene: ENSG00000101849_Expression, Epoch 10/100, Training Loss: 0.5899533629417419, Validation Loss: 0.39625248312950134\n",
            "Gene: ENSG00000101849_Expression, Epoch 20/100, Training Loss: 0.4981662929058075, Validation Loss: 0.3798815906047821\n",
            "Gene: ENSG00000101849_Expression, Epoch 30/100, Training Loss: 0.466269850730896, Validation Loss: 0.3689727485179901\n",
            "Gene: ENSG00000101849_Expression, Epoch 40/100, Training Loss: 0.44178375601768494, Validation Loss: 0.3548736274242401\n",
            "Gene: ENSG00000101849_Expression, Epoch 50/100, Training Loss: 0.42262786626815796, Validation Loss: 0.34005871415138245\n",
            "Gene: ENSG00000101849_Expression, Epoch 60/100, Training Loss: 0.40837880969047546, Validation Loss: 0.3270868957042694\n",
            "Gene: ENSG00000101849_Expression, Epoch 70/100, Training Loss: 0.3923957645893097, Validation Loss: 0.3158189654350281\n",
            "Gene: ENSG00000101849_Expression, Epoch 80/100, Training Loss: 0.3813728988170624, Validation Loss: 0.3053607940673828\n",
            "Gene: ENSG00000101849_Expression, Epoch 90/100, Training Loss: 0.3647938370704651, Validation Loss: 0.29583972692489624\n",
            "Gene: ENSG00000101849_Expression, Epoch 100/100, Training Loss: 0.35537397861480713, Validation Loss: 0.2882794737815857\n",
            "Gene: ENSG00000047644_Expression, Epoch 10/100, Training Loss: 0.345357209444046, Validation Loss: 0.1499125212430954\n",
            "Gene: ENSG00000047644_Expression, Epoch 20/100, Training Loss: 0.2629576325416565, Validation Loss: 0.14487917721271515\n",
            "Gene: ENSG00000047644_Expression, Epoch 30/100, Training Loss: 0.22753635048866272, Validation Loss: 0.13209761679172516\n",
            "Gene: ENSG00000047644_Expression, Epoch 40/100, Training Loss: 0.2150646448135376, Validation Loss: 0.12369661033153534\n",
            "Gene: ENSG00000047644_Expression, Epoch 50/100, Training Loss: 0.20050577819347382, Validation Loss: 0.11881284415721893\n",
            "Gene: ENSG00000047644_Expression, Epoch 60/100, Training Loss: 0.19285914301872253, Validation Loss: 0.11478304117918015\n",
            "Gene: ENSG00000047644_Expression, Epoch 70/100, Training Loss: 0.18088799715042114, Validation Loss: 0.109984390437603\n",
            "Gene: ENSG00000047644_Expression, Epoch 80/100, Training Loss: 0.1723167449235916, Validation Loss: 0.10603392869234085\n",
            "Gene: ENSG00000047644_Expression, Epoch 90/100, Training Loss: 0.16996963322162628, Validation Loss: 0.10315319895744324\n",
            "Gene: ENSG00000047644_Expression, Epoch 100/100, Training Loss: 0.16016410291194916, Validation Loss: 0.10028435289859772\n",
            "Gene: ENSG00000073464_Expression, Epoch 10/100, Training Loss: 0.3799247145652771, Validation Loss: 0.2268410474061966\n",
            "Gene: ENSG00000073464_Expression, Epoch 20/100, Training Loss: 0.30506473779678345, Validation Loss: 0.22390222549438477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 3 - Updated code to use a single model to be able to learn from other genes; still working with paired data (RNA+ATAC) rather than pools of data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#comment out if it is already loaded\n",
        "df = pd.read_csv('/content/sample data - Sheet1.csv') #change file path to what you have\n",
        "\n",
        "\n",
        "class PairedCNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2, num_genes=5):\n",
        "        super(PairedCNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Embedding layer for gene identity\n",
        "        self.embedding = nn.Embedding(num_genes, hidden_size)\n",
        "\n",
        "        # Linear layers for gene-specific information\n",
        "        self.fc_gene = nn.Linear(1, hidden_size)\n",
        "        self.batch_norm_gene = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout_gene = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 5)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Process gene-specific information\n",
        "        a = a.squeeze(dim=-1)\n",
        "        a_embedding = self.embedding(a.long())\n",
        "        a_embedding = a_embedding.squeeze(dim=1)\n",
        "        a_embedding = F.relu(a_embedding)\n",
        "\n",
        "        # Concatenate shared and gene-specific information\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "        self.weight_tensor = None #weight\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features) & set(self.df.columns))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        #default_value = 0\n",
        "        #self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = chromatin_accessibility_columns.union(additional_categorical_features)\n",
        "\n",
        "        # Select features and target features\n",
        "        X = self.df[feature_columns].copy()\n",
        "        y = self.df.filter(like='_Expression').copy()\n",
        "\n",
        "        # Convert categorical columns to one-hot encoding\n",
        "        X = pd.get_dummies(X, columns=additional_categorical_features)\n",
        "\n",
        "        # Train-validation-test split\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Reshape input data for CNN\n",
        "        self.X_train_tensor = torch.tensor(X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "        # Give less weight to missing values\n",
        "        missing_mask_train = X_train.isnull().astype(int)\n",
        "        weight_tensor = torch.ones_like(self.y_train_tensor)  # Initialize with ones\n",
        "        weight_tensor[missing_mask_train.values] = 0.5  # Assign lower weights to missing values\n",
        "        self.weight_tensor = weight_tensor\n",
        "\n",
        "        self.X_val_tensor = torch.tensor(X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "    def train_model(self, hidden_size=128, dropout_rate=0.2, epochs=150):\n",
        "        input_size = self.X_train_tensor.shape[1]\n",
        "\n",
        "        model = PairedCNNModel(input_size=input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Store the training and validation loss for plotting\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(self.X_train_tensor, self.X_train_tensor[:, -1:, -1:])\n",
        "            #loss = criterion(outputs, self.y_train_tensor)\n",
        "            loss = torch.mean(self.weight_tensor * (outputs - self.y_train_tensor) ** 2) #loss with weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(self.X_val_tensor, self.X_val_tensor[:, -1:, -1:])\n",
        "                val_loss = criterion(val_outputs, self.y_val_tensor)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "        # Plot the training and validation loss\n",
        "        plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        if self.model is None:\n",
        "            print(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return\n",
        "\n",
        "        model = self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(self.X_test_tensor, self.X_test_tensor[:, -1:, :])\n",
        "            test_loss = criterion(test_outputs, self.y_test_tensor)\n",
        "\n",
        "        print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "        # Print real-world and predicted gene expression for the first 10 cells\n",
        "        real_gene_expression = self.y_test_tensor[:10].detach().numpy()\n",
        "        predicted_gene_expression = test_outputs[:10].detach().numpy()\n",
        "\n",
        "        print(\"Real Gene Expression for the First 10 Cells:\")\n",
        "        print(real_gene_expression)\n",
        "\n",
        "        print(\"\\nPredicted Gene Expression for the First 10 Cells:\")\n",
        "        print(predicted_gene_expression)\n",
        "\n",
        "\n",
        "        return test_loss.item()\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "gene_predictor.train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4YERhxFh-Jwr",
        "outputId": "c2d87c92-1d30-4f80-b15d-4b1e84aae226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/150, Training Loss: 0.3301289975643158, Validation Loss: 0.1994340717792511\n",
            "Epoch 20/150, Training Loss: 0.25373005867004395, Validation Loss: 0.18900097906589508\n",
            "Epoch 30/150, Training Loss: 0.22213484346866608, Validation Loss: 0.17212766408920288\n",
            "Epoch 40/150, Training Loss: 0.19569557905197144, Validation Loss: 0.16460579633712769\n",
            "Epoch 50/150, Training Loss: 0.1821512132883072, Validation Loss: 0.15627677738666534\n",
            "Epoch 60/150, Training Loss: 0.16936413943767548, Validation Loss: 0.15129485726356506\n",
            "Epoch 70/150, Training Loss: 0.16119104623794556, Validation Loss: 0.14480462670326233\n",
            "Epoch 80/150, Training Loss: 0.1545097529888153, Validation Loss: 0.14215940237045288\n",
            "Epoch 90/150, Training Loss: 0.1494482457637787, Validation Loss: 0.14025235176086426\n",
            "Epoch 100/150, Training Loss: 0.14527800679206848, Validation Loss: 0.13893912732601166\n",
            "Epoch 110/150, Training Loss: 0.1439693570137024, Validation Loss: 0.13825616240501404\n",
            "Epoch 120/150, Training Loss: 0.14142313599586487, Validation Loss: 0.13751159608364105\n",
            "Epoch 130/150, Training Loss: 0.13966283202171326, Validation Loss: 0.137372225522995\n",
            "Epoch 140/150, Training Loss: 0.1381247341632843, Validation Loss: 0.13727955520153046\n",
            "Epoch 150/150, Training Loss: 0.13604967296123505, Validation Loss: 0.13710254430770874\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "Test Loss: 0.13294291496276855\n",
            "Real Gene Expression for the First 10 Cells:\n",
            "[[0.        1.0986123 0.        1.0986123 0.       ]\n",
            " [0.6931472 0.        0.        0.        0.       ]\n",
            " [0.6931472 0.        0.        0.        0.       ]\n",
            " [0.6931472 0.        0.        0.        0.       ]\n",
            " [0.6931472 0.        0.        0.        0.       ]\n",
            " [1.0986123 0.        0.        0.        0.       ]\n",
            " [1.9459101 0.        0.        0.        0.       ]\n",
            " [1.3862944 0.        0.        0.        0.       ]\n",
            " [0.        0.        0.        0.        0.       ]\n",
            " [0.        0.6931472 0.        0.        0.       ]]\n",
            "\n",
            "Predicted Gene Expression for the First 10 Cells:\n",
            "[[0.16249423 0.11377338 0.06379608 0.09678629 0.04665623]\n",
            " [0.33109245 0.28049487 0.07985853 0.13475958 0.03935435]\n",
            " [0.16249423 0.11377338 0.06379608 0.09678629 0.04665623]\n",
            " [0.4047278  0.15497826 0.08090683 0.12029871 0.04280663]\n",
            " [0.32601908 0.17364144 0.08829386 0.08895067 0.06694054]\n",
            " [0.33109245 0.28049487 0.07985853 0.13475958 0.03935435]\n",
            " [0.3610103  0.16950548 0.07234577 0.09058841 0.03919321]\n",
            " [0.2837822  0.12163474 0.05529607 0.12928492 0.0162103 ]\n",
            " [0.3229485  0.11164698 0.06057337 0.10935619 0.01287289]\n",
            " [0.25677735 0.15375796 0.06463362 0.11463343 0.02610113]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13294291496276855"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is limited in that there are a number of zeroes and the model is not able to predict them well. It is also only based on 5 genes, so it may generalize better with more data. Next step is to implement weights to account for the zeroes."
      ],
      "metadata": {
        "id": "NDCrb4fEq-gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 4 - Training and validating model on dataset (oligodendrocytes, all genes on X chr)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#comment out if it is already loaded\n",
        "df = pd.read_csv('/finaldataset_oligodendrocytes.csv') #change file path to what you have\n",
        "\n",
        "\n",
        "class PairedCNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2, num_genes=652):\n",
        "        super(PairedCNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(10432, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Embedding layer for gene identity\n",
        "        self.embedding = nn.Embedding(num_genes, hidden_size)\n",
        "\n",
        "        # Linear layers for gene-specific information\n",
        "        self.fc_gene = nn.Linear(1, hidden_size)\n",
        "        self.batch_norm_gene = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout_gene = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 652)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Process gene-specific information\n",
        "        a = a.squeeze(dim=-1)\n",
        "        a_embedding = self.embedding(a.long())\n",
        "        a_embedding = a_embedding.squeeze(dim=1)\n",
        "        a_embedding = F.relu(a_embedding)\n",
        "\n",
        "        # Concatenate shared and gene-specific information\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "        self.weight_tensor = None #weight\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features) & set(self.df.columns))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        #default_value = 0\n",
        "        #self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = chromatin_accessibility_columns.union(additional_categorical_features)\n",
        "\n",
        "        # Select features and target features\n",
        "        X = self.df[feature_columns].copy()\n",
        "        y = self.df.filter(like='_Expression').copy()\n",
        "\n",
        "        # Convert categorical columns to one-hot encoding\n",
        "        X = pd.get_dummies(X, columns=additional_categorical_features)\n",
        "\n",
        "        # Train-validation-test split\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Reshape input data for CNN\n",
        "        self.X_train_tensor = torch.tensor(X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "        # Give less weight to missing values\n",
        "        missing_mask_train = X_train.isnull().astype(int)\n",
        "        weight_tensor = torch.ones_like(self.y_train_tensor)  # Initialize with ones\n",
        "        weight_tensor[missing_mask_train.values] = 0.5  # Assign lower weights to missing values\n",
        "        self.weight_tensor = weight_tensor\n",
        "\n",
        "        self.X_val_tensor = torch.tensor(X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "        # Save the test dataset to a CSV file\n",
        "        test_dataset = pd.concat([self.X_test_tensor.squeeze(), self.y_test_tensor.squeeze()], axis=1)\n",
        "        test_dataset.columns = feature_columns.tolist() + y.columns.tolist()\n",
        "        test_dataset.to_csv(save_test_csv, index=False)\n",
        "\n",
        "        print(f'Test dataset saved to {save_test_csv}')\n",
        "\n",
        "    def train_model(self, hidden_size=128, dropout_rate=0.2, epochs=100):\n",
        "        print(\"Welcome. Using scGenePredix to train the model.\")\n",
        "        input_size = self.X_train_tensor.shape[1]\n",
        "\n",
        "        model = PairedCNNModel(input_size=input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Store the training and validation loss for plotting\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        print(\"Training in progress...\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(self.X_train_tensor, self.X_train_tensor[:, -1:, -1:])\n",
        "            #loss = criterion(outputs, self.y_train_tensor)\n",
        "            loss = torch.mean(self.weight_tensor * (outputs - self.y_train_tensor) ** 2) #loss with weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(self.X_val_tensor, self.X_val_tensor[:, -1:, -1:])\n",
        "                val_loss = criterion(val_outputs, self.y_val_tensor)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "              print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "        # Plot the training and validation loss\n",
        "        plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "        torch.save(model.state_dict(), 'trained_model_oligodendroyctes.pth')\n",
        "        print(\"Trained model saved to 'trained_model_oligodendroyctes.pth'\")\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        if self.model is None:\n",
        "            print(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return\n",
        "\n",
        "        model = self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(self.X_test_tensor, self.X_test_tensor[:, -1:, :])\n",
        "            test_loss = criterion(test_outputs, self.y_test_tensor)\n",
        "\n",
        "        print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "        # Print real-world and predicted gene expression for the first 10 cells\n",
        "        real_gene_expression = self.y_test_tensor.detach().numpy()\n",
        "        predicted_gene_expression = test_outputs.detach().numpy()\n",
        "\n",
        "        #print(\"Real Gene Expression for the First 10 Cells:\")\n",
        "        #print(real_gene_expression)\n",
        "\n",
        "        #print(\"\\nPredicted Gene Expression for the First 10 Cells:\")\n",
        "        #print(predicted_gene_expression)\n",
        "\n",
        "\n",
        "        return test_loss.item()\n",
        "\n",
        "# Load the data\n",
        "#df = pd.read_csv('sample data - Sheet1.csv')\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "gene_predictor.train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "eiYSSPUONLgC",
        "outputId": "caac154f-6e97-4fcb-f6cc-77e606a401ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome. Using scGenePredix to train the model.\n",
            "Training in progress...\n",
            "Epoch 10/100, Training Loss: 0.207123801112175, Validation Loss: 0.1674545407295227\n",
            "Epoch 20/100, Training Loss: 0.15232199430465698, Validation Loss: 0.18257339298725128\n",
            "Epoch 30/100, Training Loss: 0.13260908424854279, Validation Loss: 0.10554434359073639\n",
            "Epoch 40/100, Training Loss: 0.11794862151145935, Validation Loss: 0.0719032734632492\n",
            "Epoch 50/100, Training Loss: 0.1080954298377037, Validation Loss: 0.06256715208292007\n",
            "Epoch 70/100, Training Loss: 0.08971282839775085, Validation Loss: 0.0525813102722168\n",
            "Epoch 80/100, Training Loss: 0.08317898958921432, Validation Loss: 0.049109846353530884\n",
            "Epoch 90/100, Training Loss: 0.07539478689432144, Validation Loss: 0.04517226293683052\n",
            "Epoch 100/100, Training Loss: 0.07001281529664993, Validation Loss: 0.04196983948349953\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "Trained model saved to 'trained_model_oligodendroyctes.pth'\n",
            "Test Loss: 0.04126623272895813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04126623272895813"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 5 - Training and validating model on final dataset after incorporating start, stop coordinates (oligodendrocytes, all genes on X chr)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#comment out if it is already loaded\n",
        "df = pd.read_csv('finaldataset_oligodendrocytes.csv')\n",
        "\n",
        "\n",
        "class PairedCNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2, num_genes=652):\n",
        "        super(PairedCNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(31296, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Embedding layer for gene identity\n",
        "        self.embedding = nn.Embedding(num_genes, hidden_size)\n",
        "\n",
        "        # Linear layers for gene-specific information\n",
        "        self.fc_gene = nn.Linear(1, hidden_size)\n",
        "        self.batch_norm_gene = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout_gene = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 652)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Process gene-specific information\n",
        "        a = a.squeeze(dim=-1)\n",
        "        a_embedding = self.embedding(a.long())\n",
        "        a_embedding = a_embedding.squeeze(dim=1)\n",
        "        a_embedding = F.relu(a_embedding)\n",
        "\n",
        "        # Concatenate shared and gene-specific information\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "        self.weight_tensor = None #weight\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        #default_value = 0\n",
        "        #self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Identify gene start columns\n",
        "        start_columns = self.df.filter(like='_start_position').columns\n",
        "\n",
        "        # Identify gene stop columns\n",
        "        stop_columns = self.df.filter(like='_end_position').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = (chromatin_accessibility_columns.union(additional_categorical_features).union(start_columns).union(stop_columns)\n",
        ")\n",
        "        # Select features and target features\n",
        "        X = self.df[feature_columns].copy()\n",
        "        y = self.df.filter(like='_Expression').copy()\n",
        "\n",
        "        # Convert categorical columns to one-hot encoding\n",
        "        X = pd.get_dummies(X, columns=additional_categorical_features)\n",
        "\n",
        "        # Train-validation-test split\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Reshape input data for CNN\n",
        "        self.X_train_tensor = torch.tensor(X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "        # Give less weight to missing values\n",
        "        missing_mask_train = X_train.isnull().astype(int)\n",
        "        weight_tensor = torch.ones_like(self.y_train_tensor)  # Initialize with ones\n",
        "        weight_tensor[missing_mask_train.values] = 0.5  # Assign lower weights to missing values\n",
        "        self.weight_tensor = weight_tensor\n",
        "\n",
        "        self.X_val_tensor = torch.tensor(X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "    def train_model(self, hidden_size=128, dropout_rate=0.2, epochs=100):\n",
        "        print(\"Welcome. Using scGenePredix to train the model.\")\n",
        "        input_size = self.X_train_tensor.shape[1]\n",
        "\n",
        "        model = PairedCNNModel(input_size=input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Store the training and validation loss for plotting\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        print(\"Training in progress...\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(self.X_train_tensor, self.X_train_tensor[:, -1:, -1:])\n",
        "            #loss = criterion(outputs, self.y_train_tensor)\n",
        "            loss = torch.mean(self.weight_tensor * (outputs - self.y_train_tensor) ** 2) #loss with weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(self.X_val_tensor, self.X_val_tensor[:, -1:, -1:])\n",
        "                val_loss = criterion(val_outputs, self.y_val_tensor)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "              print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "        # Plot the training and validation loss\n",
        "        plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.ylim(0, 0.5)  # Adjust y-axis limits\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "        torch.save(model.state_dict(), 'trained_model_oligodendroyctes.pth')\n",
        "        print(\"Trained model saved to 'trained_model_oligodendroyctes.pth'\")\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        if self.model is None:\n",
        "            print(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return\n",
        "\n",
        "        model = self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(self.X_test_tensor, self.X_test_tensor[:, -1:, :])\n",
        "            test_loss = criterion(test_outputs, self.y_test_tensor)\n",
        "\n",
        "        print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "        return test_loss.item()\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "gene_predictor.train_model()\n",
        "\n",
        "# Evaluate the model\n",
        "gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "XoqJ3SMDs_Go",
        "outputId": "16384d4f-574d-4c72-a22c-2a24b4b5bd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome. Using scGenePredix to train the model.\n",
            "Training in progress...\n",
            "Epoch 10/100, Training Loss: 0.18405351042747498, Validation Loss: 6.907622814178467\n",
            "Epoch 20/100, Training Loss: 0.1112041249871254, Validation Loss: 0.15610316395759583\n",
            "Epoch 30/100, Training Loss: 0.09360269457101822, Validation Loss: 0.09680071473121643\n",
            "Epoch 40/100, Training Loss: 0.08381722122430801, Validation Loss: 0.07543222606182098\n",
            "Epoch 50/100, Training Loss: 0.07549668103456497, Validation Loss: 0.06207575649023056\n",
            "Epoch 60/100, Training Loss: 0.06766776740550995, Validation Loss: 0.053892988711595535\n",
            "Epoch 70/100, Training Loss: 0.061119090765714645, Validation Loss: 0.04932181164622307\n",
            "Epoch 80/100, Training Loss: 0.0557955764234066, Validation Loss: 0.045482710003852844\n",
            "Epoch 90/100, Training Loss: 0.05129629373550415, Validation Loss: 0.04260258004069328\n",
            "Epoch 100/100, Training Loss: 0.048094913363456726, Validation Loss: 0.04014135152101517\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcUklEQVR4nO3deXRTdf4+8OcmabYu6b5QCmUvILRYFgsuONYBdBAQlXEYQQZxVECRr6MyCLhXx41RGFFmgHEF8afIKIJQQWVR9r3s0BboQiltuiVpkvv74yZpC6V0SXKb9Hmdc0/S5Cb33XsY+8xnFURRFEFERETkJxRyF0BERETkTgw3RERE5FcYboiIiMivMNwQERGRX2G4ISIiIr/CcENERER+heGGiIiI/ArDDREREfkVhhsiIiLyKww3RERE5FdaRbhZuHAhEhMTodVqMWjQIGzfvv2q5y5btgyCINQ5tFqtF6slIiKi1kz2cLNixQrMnDkT8+bNw+7du5GcnIxhw4ahsLDwqp8JCQlBXl6e68jOzvZixURERNSayR5u3n77bUyZMgWTJk1Cr169sGjRIuj1eixZsuSqnxEEAbGxsa4jJibGixUTERFRa6aS8+IWiwW7du3CrFmzXK8pFAqkp6dj27ZtV/1ceXk5OnbsCLvdjuuvvx6vvvoqevfuXe+5ZrMZZrPZ9bPdbkdxcTEiIiIgCIL7fhkiIiLyGFEUUVZWhnbt2kGhaLhtRtZwU1RUBJvNdkXLS0xMDI4cOVLvZ3r06IElS5agb9++KC0txZtvvonBgwfj0KFDaN++/RXnZ2Rk4IUXXvBI/URERORdubm59f69r03WcNMcaWlpSEtLc/08ePBg9OzZEx988AFeeumlK86fNWsWZs6c6fq5tLQUHTp0QG5uLkJCQrxSc5v0/hCgJBv48yogob/c1RARkY8zGo1ISEhAcHDwNc+VNdxERkZCqVSioKCgzusFBQWIjY1t1HcEBASgX79+OHHiRL3vazQaaDSaK14PCQlhuPGkID1QJQB6FcD7TEREbtKYISWyDihWq9VITU1FZmam6zW73Y7MzMw6rTMNsdlsOHDgAOLi4jxVJjWHUi09Wi3y1kFERG2O7N1SM2fOxMSJE9G/f38MHDgQ8+fPR0VFBSZNmgQAmDBhAuLj45GRkQEAePHFF3HDDTega9euKCkpwRtvvIHs7Gw89NBDcv4adDmVY+0hq0neOoiIqM2RPdyMGzcOFy5cwNy5c5Gfn4+UlBSsXbvWNcg4JyenzqjoS5cuYcqUKcjPz0dYWBhSU1OxdetW9OrVS65fgeqjcnQF2swNn0dERORmgiiKotxFeJPRaITBYEBpaSnH3HjSx3cDJzOB0YuAlPvlroaI3Mxms6G6ulruMsjPqNXqq07zbsrfb9lbbshPseWGyC+Jooj8/HyUlJTIXQr5IYVCgU6dOkGtVrfoexhuyDOc4cbKcEPkT5zBJjo6Gnq9nouhktvY7XacP38eeXl56NChQ4v+bTHckGcoGW6I/I3NZnMFm4iICLnLIT8UFRWF8+fPw2q1IiAgoNnfI/veUuSnVI4mRXZLEfkN5xgbvV4vcyXkr5zdUTabrUXfw3BDnuFqueE6N0T+hl1R5Cnu+rfFbik3yS814dv956FXq/CnQR3kLkd+rjE3XOeGiIi8iy03bnKupBIvf5eFRT+dlLuU1sE1W4otN0TkfxITEzF//vxGn79p0yYIgsBZZl7CcOMm2gAlAMBU3bJ+Qr/BAcVE1AoIgtDg8fzzzzfre3fs2IGHH3640ecPHjwYeXl5MBgMzbpeYzFESdgt5SYMN5dxDShmyw0RyScvL8/1fMWKFZg7dy6OHj3qei0oKMj1XBRF2Gw2qFTX/tMYFRXVpDrUanWjN4SmlmPLjZu4wo3VLnMlrQT3liKiViA2NtZ1GAwGCILg+vnIkSMIDg7G999/j9TUVGg0GmzevBknT57EqFGjEBMTg6CgIAwYMAAbNmyo872Xd0sJgoB///vfGDNmDPR6Pbp164bVq1e73r+8RWXZsmUIDQ3FunXr0LNnTwQFBWH48OF1wpjVasXjjz+O0NBQRERE4JlnnsHEiRMxevToZt+PS5cuYcKECQgLC4Ner8eIESNw/Phx1/vZ2dkYOXIkwsLCEBgYiN69e2PNmjWuz44fPx5RUVHQ6XTo1q0bli5d2uxaPInhxk20KulWWqx22OxtakeL+rl2BWe3FJG/EkURlRarLIc7dw569tln8dprryErKwt9+/ZFeXk57rjjDmRmZmLPnj0YPnw4Ro4ciZycnAa/54UXXsB9992H/fv344477sD48eNRXFx81fMrKyvx5ptv4uOPP8bPP/+MnJwcPPXUU673X3/9dXz66adYunQptmzZAqPRiFWrVrXod33wwQexc+dOrF69Gtu2bYMoirjjjjtc0/ynTp0Ks9mMn3/+GQcOHMDrr7/uat2aM2cODh8+jO+//x5ZWVl4//33ERkZ2aJ6PIXdUm7ibLkBALPVBr26jd9aDigm8ntV1Tb0mrtOlmsffnGY2/47++KLL+L22293/RweHo7k5GTXzy+99BK+/vprrF69GtOmTbvq9zz44IO4/35pL71XX30V7777LrZv347hw4fXe351dTUWLVqELl26AACmTZuGF1980fX+e++9h1mzZmHMmDEAgAULFrhaUZrj+PHjWL16NbZs2YLBgwcDAD799FMkJCRg1apVuPfee5GTk4OxY8eiT58+AIDOnTu7Pp+Tk4N+/fqhf//+AKTWq9aKLTduUjvcmKrZNcUBxUTkK5x/rJ3Ky8vx1FNPoWfPnggNDUVQUBCysrKu2XLTt29f1/PAwECEhISgsLDwqufr9XpXsAGAuLg41/mlpaUoKCjAwIEDXe8rlUqkpqY26XerLSsrCyqVCoMGDXK9FhERgR49eiArKwsA8Pjjj+Pll1/GkCFDMG/ePOzfv9917qOPPorly5cjJSUFTz/9NLZu3drsWjytjTcvuI9SISBAKaDaJnJQMVAzoJjhhshv6QKUOPziMNmu7S6BgYF1fn7qqaewfv16vPnmm+jatSt0Oh3uueceWCwNt0Rfvl2AIAiw26/+f3brO9+d3W3N8dBDD2HYsGH47rvv8MMPPyAjIwNvvfUWpk+fjhEjRiA7Oxtr1qzB+vXrcdttt2Hq1Kl48803Za25Pmy5cSPOmKrFOaCY2y8Q+S1BEKBXq2Q5PLlK8pYtW/Dggw9izJgx6NOnD2JjY3HmzBmPXa8+BoMBMTEx2LFjh+s1m82G3bt3N/s7e/bsCavVit9++8312sWLF3H06FH06tXL9VpCQgIeeeQRfPXVV/i///s/LF682PVeVFQUJk6ciE8++QTz58/Hhx9+2Ox6PIktN26kDVCizGRltxRQa0Axx9wQkW/p1q0bvvrqK4wcORKCIGDOnDkNtsB4yvTp05GRkYGuXbsiKSkJ7733Hi5dutSoYHfgwAEEBwe7fhYEAcnJyRg1ahSmTJmCDz74AMHBwXj22WcRHx+PUaNGAQBmzJiBESNGoHv37rh06RI2btyInj17AgDmzp2L1NRU9O7dG2azGd9++63rvdaG4caNtAFSQ5jJypabmgHFbLkhIt/y9ttv4y9/+QsGDx6MyMhIPPPMMzAajV6v45lnnkF+fj4mTJgApVKJhx9+GMOGDYNSee0uuZtvvrnOz0qlElarFUuXLsUTTzyBP/zhD7BYLLj55puxZs0aVxeZzWbD1KlTcfbsWYSEhGD48OF45513AEhr9cyaNQtnzpyBTqfDTTfdhOXLl7v/F3cDQZS7g8/LjEYjDAYDSktLERIS4tbvvv3tn3C8sByfPTQIg7u2zulxXnN2F/Dv3wGGBODJg3JXQ0RuYDKZcPr0aXTq1AlarVbuctocu92Onj174r777sNLL70kdzke0dC/sab8/WbLjRvVLOTHlpuajTPZckNE1BzZ2dn44YcfcMstt8BsNmPBggU4ffo0/vSnP8ldWqvHAcVu5OqW4pgbdksREbWQQqHAsmXLMGDAAAwZMgQHDhzAhg0bWu04l9aELTduxNlStXBAMRFRiyQkJGDLli1yl+GT2HLjRjXhhi03Nd1SJqBtDesiIiKZMdy4EVtuanGGG4iA3SprKURE1LYw3LiRc/NMDihGzfYLAAcVExGRVzHcuJGr5cbCcFPTcgNunklERF7FcONGNYv4ccwNFEpAcCw0ZTXJWwsREbUpDDduxDE3l3HuL8VuKSIi8iKGGzdiuLmMc2dwdksRkY8bOnQoZsyY4fo5MTER8+fPb/AzgiBg1apVLb62u76nLWG4cSNOBb+Ms+WmukreOoiozRo5ciSGDx9e73u//PILBEHA/v37m/y9O3bswMMPP9zS8up4/vnnkZKScsXreXl5GDFihFuvdblly5YhNDTUo9fwJoYbN6pZoZgtNwCAAL30WF0pbx1E1GZNnjwZ69evx9mzZ694b+nSpejfvz/69u3b5O+NioqCXq93R4nXFBsbC41Gc+0TyYXhxo20KufeUmy5AQCoA6VHC8MNEcnjD3/4A6KiorBs2bI6r5eXl2PlypWYPHkyLl68iPvvvx/x8fHQ6/Xo06cPPv/88wa/9/JuqePHj+Pmm2+GVqtFr169sH79+is+88wzz6B79+7Q6/Xo3Lkz5syZg+rqagBSy8kLL7yAffv2QRAECILgqvnybqkDBw7gd7/7HXQ6HSIiIvDwww+jvLzc9f6DDz6I0aNH480330RcXBwiIiIwdepU17WaIycnB6NGjUJQUBBCQkJw3333oaCgwPX+vn37cOuttyI4OBghISFITU3Fzp07AUh7ZI0cORJhYWEIDAxE7969sWbNmmbX0hjcfsGNOBX8Muog6dFS3vB5ROSbRFG+ltkAPSAI1zxNpVJhwoQJWLZsGWbPng3B8ZmVK1fCZrPh/vvvR3l5OVJTU/HMM88gJCQE3333HR544AF06dIFAwcOvOY17HY77r77bsTExOC3335DaWlpnfE5TsHBwVi2bBnatWuHAwcOYMqUKQgODsbTTz+NcePG4eDBg1i7di02bNgAADAYDFd8R0VFBYYNG4a0tDTs2LEDhYWFeOihhzBt2rQ6AW7jxo2Ii4vDxo0bceLECYwbNw4pKSmYMmXKNX+f+n4/Z7D56aefYLVaMXXqVIwbNw6bNm0CAIwfPx79+vXD+++/D6VSib179yIgIAAAMHXqVFgsFvz8888IDAzE4cOHERQU1OQ6moLhxo1qpoIz3AAA1I4mW0uFvHUQkWdUVwKvtpPn2n8/X9M6fA1/+ctf8MYbb+Cnn37C0KFDAUhdUmPHjoXBYIDBYMBTTz3lOn/69OlYt24dvvjii0aFmw0bNuDIkSNYt24d2rWT7serr756xTiZ5557zvU8MTERTz31FJYvX46nn34aOp0OQUFBUKlUiI2Nveq1PvvsM5hMJnz00UcIDJR+/wULFmDkyJF4/fXXERMTAwAICwvDggULoFQqkZSUhDvvvBOZmZnNCjeZmZk4cOAATp8+jYSEBADARx99hN69e2PHjh0YMGAAcnJy8Le//Q1JSUkAgG7durk+n5OTg7Fjx6JPnz4AgM6dOze5hqZit5QbcbbUZZz/4eGYGyKSUVJSEgYPHowlS5YAAE6cOIFffvkFkydPBgDYbDa89NJL6NOnD8LDwxEUFIR169YhJyenUd+flZWFhIQEV7ABgLS0tCvOW7FiBYYMGYLY2FgEBQXhueeea/Q1al8rOTnZFWwAYMiQIbDb7Th69Kjrtd69e0OpVLp+jouLQ2FhYZOuVfuaCQkJrmADAL169UJoaCiysrIAADNnzsRDDz2E9PR0vPbaazh58qTr3Mcffxwvv/wyhgwZgnnz5jVrAHdTseXGjWoGFHPMDQAgwDnmht1SRH4pQC+1oMh17SaYPHkypk+fjoULF2Lp0qXo0qULbrnlFgDAG2+8gX/+85+YP38++vTpg8DAQMyYMQMWi/uWsdi2bRvGjx+PF154AcOGDYPBYMDy5cvx1ltvue0atTm7hJwEQYDd7rm/Tc8//zz+9Kc/4bvvvsP333+PefPmYfny5RgzZgweeughDBs2DN999x1++OEHZGRk4K233sL06dM9Vg9bbtyILTeX4YBiIv8mCNL/zuU4GjHeprb77rsPCoUCn332GT766CP85S9/cY2/2bJlC0aNGoU///nPSE5ORufOnXHs2LFGf3fPnj2Rm5uLvLw812u//vprnXO2bt2Kjh07Yvbs2ejfvz+6deuG7OzsOueo1WrYbA3//ejZsyf27duHioqa7v4tW7ZAoVCgR48eja65KZy/X25uruu1w4cPo6SkBL169XK91r17dzz55JP44YcfcPfdd2Pp0qWu9xISEvDII4/gq6++wv/93/9h8eLFHqnVieHGjRhuLuMKNxxzQ0TyCgoKwrhx4zBr1izk5eXhwQcfdL3XrVs3rF+/Hlu3bkVWVhb++te/1pkJdC3p6eno3r07Jk6ciH379uGXX37B7Nmz65zTrVs35OTkYPny5Th58iTeffddfP3113XOSUxMxOnTp7F3714UFRXBbL5ydffx48dDq9Vi4sSJOHjwIDZu3Ijp06fjgQcecI23aS6bzYa9e/fWObKyspCeno4+ffpg/Pjx2L17N7Zv344JEybglltuQf/+/VFVVYVp06Zh06ZNyM7OxpYtW7Bjxw707NkTADBjxgysW7cOp0+fxu7du7Fx40bXe57CcONGrnDDqeASV7hhtxQRyW/y5Mm4dOkShg0bVmd8zHPPPYfrr78ew4YNw9ChQxEbG4vRo0c3+nsVCgW+/vprVFVVYeDAgXjooYfwyiuv1DnnrrvuwpNPPolp06YhJSUFW7duxZw5c+qcM3bsWAwfPhy33noroqKi6p2OrtfrsW7dOhQXF2PAgAG45557cNttt2HBggVNuxn1KC8vR79+/eocI0eOhCAI+OabbxAWFoabb74Z6enp6Ny5M1asWAEAUCqVuHjxIiZMmIDu3bvjvvvuw4gRI/DCCy8AkELT1KlT0bNnTwwfPhzdu3fHv/71rxbX2xBBFEXRo1doZYxGIwwGA0pLSxESEuLW775Ybkbqy9IUvpOv3gGlomnNpn5n20Jg3d+BPvcCY/8tdzVE1EImkwmnT59Gp06doNVq5S6H/FBD/8aa8vebLTdu5Gy5AQAzp4OzW4qIiGTBcONGtcMNZ0yBs6WIiEgWDDdupFQIUCu5v5QLZ0sREZEMGG7cTMPNM2uwW4qIiGTAcONmNdPB2S3FcEPkn9rYPBTyInf922K4cTPuL1WLa/sFhhsif+Bc9baykl3N5BnOVaFrbx3RHNx+wc20Ku4M7hLAjTOJ/IlSqURoaKhrjyK9Xu9a5Zeopex2Oy5cuAC9Xg+VqmXxhOHGzWoW8mO4gdqxpb3VBNhtgKJlSZyI5Ofcsbq5mzASNUShUKBDhw4tDs0MN27GzTNrcXZLAVLrjda9iyYSkfcJgoC4uDhER0ejurpa7nLIz6jVaigULR8xw3DjZtxfqhaVBhAUgGhnuCHyM0qlssXjIog8hQOK3YyzpWoRhJquqWoOQCQiIu9guHEzttxchptnEhGRlzHcuJlWJd3SKoYbCWdMERGRlzHcuJmz5cbMcCPhFgxERORlDDduVrOIH8fcAGC3FBEReR3DjZtxzM1lXKsUs+WGiIi8g+HGzRhuLsP9pYiIyMsYbtyMU8EvE8BuKSIi8i6GGzerWaGYLTcAOKCYiIi8juHGzZwbZ3IquAO7pYiIyMsYbtysZio4u6UAcLYUERF5HcONm9VMBWfLDQDOliIiIq9juHEzzpa6DFcoJiIiL2O4cTPOlrqMc+NMdksREZGXtIpws3DhQiQmJkKr1WLQoEHYvn17oz63fPlyCIKA0aNHe7bAJuBsqctwthQREXmZ7OFmxYoVmDlzJubNm4fdu3cjOTkZw4YNQ2FhYYOfO3PmDJ566incdNNNXqq0cdgtdRk1u6WIiMi7ZA83b7/9NqZMmYJJkyahV69eWLRoEfR6PZYsWXLVz9hsNowfPx4vvPACOnfu7MVqr43dUpdxdktVM9wQEZF3yBpuLBYLdu3ahfT0dNdrCoUC6enp2LZt21U/9+KLLyI6OhqTJ0++5jXMZjOMRmOdw5O0KumWWmx22OyiR6/lE7jODREReZms4aaoqAg2mw0xMTF1Xo+JiUF+fn69n9m8eTP+85//YPHixY26RkZGBgwGg+tISEhocd0NcbbcAICZ08E5W4qIiLxO9m6ppigrK8MDDzyAxYsXIzIyslGfmTVrFkpLS11Hbm6uR2usHW7YNYWabimrCbAz7BERkeep5Lx4ZGQklEolCgoK6rxeUFCA2NjYK84/efIkzpw5g5EjR7pes9ulAKFSqXD06FF06dKlzmc0Gg00Go0Hqq+fUiFArVTAYrNzUDFQ0y0FSK032hD5aiEiojZB1pYbtVqN1NRUZGZmul6z2+3IzMxEWlraFecnJSXhwIED2Lt3r+u46667cOutt2Lv3r0e73JqLA2ng9dQaQDB8c+MXVNEROQFsrbcAMDMmTMxceJE9O/fHwMHDsT8+fNRUVGBSZMmAQAmTJiA+Ph4ZGRkQKvV4rrrrqvz+dDQUAC44nU5aQOUKDNZ2S0FAIIgdU2ZjdyCgYiIvEL2cDNu3DhcuHABc+fORX5+PlJSUrB27VrXIOOcnBwoFD41NMi1kB93BncI0EvhhqsUExGRF8gebgBg2rRpmDZtWr3vbdq0qcHPLlu2zP0FtZBW5dwZnOEGAKeDExGRV/lWk4iPcC3kx6ngEm7BQEREXsRw4wE1+0txzA2AWuGG3VJEROR5DDcewP2lLuMMNxxQTEREXsBw4wHcX+oyHHNDRERexHDjAWy5uUwAu6WIiMh7GG48wLl5JqeCO3BAMREReRHDjQc4W244FdxBzc0ziYjIexhuPMA1W8rKMTcAajbPZLcUERF5AcONB3DMzWU4W4qIiLyI4cYDGG4uE8BuKSIi8h6GGw/gVPDLsFuKiIi8iOHGA2pWKGbLDQDOliIiIq9iuPEA58aZnAruwNlSRETkRQw3HlAzFZzdUgBquqWqGW6IiMjzGG48oGYqOFtuAHBAMREReRXDjQdwttRluLcUERF5EcONB3C21GWc3VJWE2Bn4CMiIs9iuPEAzpa6jHNAMcDWGyIi8jiGGw9gt9RlVFpAcPxTY7ghIiIPY7jxAHZLXUYQas2Y4lo3RETkWQw3HqBVSbfVYrPDZhdlrqaVcM2Y4irFRETkWQw3HuBsuQEAM6eDSzhjioiIvIThxgNqhxt2TTlwCwYiIvIShhsPUCoEqJWcMVWHK9ywW4qIiDyL4cZDNJwOXpcz3HBAMREReRjDjYc4u6a4eaYDt2AgIiIvYbjxkJqF/DjmBkDNVHB2SxERkYcx3HiIVuXcGZwtNwA4oJiIiLyG4cZDXAv5cSq4RM1uKSIi8g6GGw/RcZXiutgtRUREXsJw4yGcLXUZzpYiIiIvYbjxEO4vdRnOliIiIi9huPEQTgW/jKtbiuGGiIg8i+HGQ5ybZ7JbyoEDiomIyEsYbjzE2XLDqeAO3DiTiIi8hOHGQ1yL+Fk55gZATbdUNcMNERF5FsONh9RMBWfLDQAOKCYiIq9huPEQDcNNXeyWIiIiL2G48RBOBb+Ms1vKagLsDHxEROQ5DDce4hxzw6ngDs7ZUgBbb4iIyKMYbjzEuXEmu6UcVFpAcPxzY7ghIiIPYrjxkJqp4OyWAgAIAhDALRiIiMjzGG48pGYqOFtuXFyDirl5JhEReQ7DjYdwKng9XOGGLTdEROQ5DDceouFsqStxCwYiIvIChhsPcXVLseWmhmvzTHZLERGR5zDceAh3Ba+HmgOKiYjI8xhuPISzperBLRiIiMgLGG48RKuSbq3FZofNLspcTSvBbikiIvIChhsPcbbcAICZ08ElrgHF7JYiIiLPYbjxkNrhhjOmHLh5JhEReQHDjYcoFQLUSs6YqoPdUkRE5AUMNx6k4XTwupwDijlbioiIPIjhxoM4Hfwy7JYiIiIvYLjxoJqF/DjmBkBNt5S5TN46iIjIrzHceJBW5Vzrhi03AABdqPRoKpGzCiIi8nMMNx7k7JbizuAOunDpsfKSvHUQEZFfY7jxIB03z6xLHyY9VhXLWwcREfk1hhsP4mypy+gc4aa6Eqg2yVsLERH5LYYbD+JsqctoDIDg+CdXxa4pIiLyDIYbD2pn0AIAjuRxdhAAQKGoab1h1xQREXkIw40HDe4aCQDYcqJI5kpaEdegYoYbIiLyDIYbD0rrEgGlQsCpogqcK6mSu5zWwdVyw24pIiLyjFYRbhYuXIjExERotVoMGjQI27dvv+q5X331Ffr374/Q0FAEBgYiJSUFH3/8sRerbbwQbQCS2xsAAJuPX5C5mlZC72i5YbcUERF5iOzhZsWKFZg5cybmzZuH3bt3Izk5GcOGDUNhYWG954eHh2P27NnYtm0b9u/fj0mTJmHSpElYt26dlytvnBu7RQEAfjnOrikA7JYiIiKPkz3cvP3225gyZQomTZqEXr16YdGiRdDr9ViyZEm95w8dOhRjxoxBz5490aVLFzzxxBPo27cvNm/e7OXKG+dGx7ibrScvwm4XZa6mFWC3FBEReZis4cZisWDXrl1IT093vaZQKJCeno5t27Zd8/OiKCIzMxNHjx7FzTffXO85ZrMZRqOxzuFN/TqEIlCtRHGFBYfzvHvtVokL+RERkYfJGm6Kiopgs9kQExNT5/WYmBjk5+df9XOlpaUICgqCWq3GnXfeiffeew+33357vedmZGTAYDC4joSEBLf+DtcSoFTghs4RAIDNnDXFLRiIiMjjZO+Wao7g4GDs3bsXO3bswCuvvIKZM2di06ZN9Z47a9YslJaWuo7c3FzvFgvgxm5S19RmjrvhgGIiIvI4lZwXj4yMhFKpREFBQZ3XCwoKEBsbe9XPKRQKdO3aFQCQkpKCrKwsZGRkYOjQoVecq9FooNFo3Fp3UznH3Ww/UwxTtc21cnGbxDE3RETkYbK23KjVaqSmpiIzM9P1mt1uR2ZmJtLS0hr9PXa7HWaz2RMlukXX6CDEhGhgsdqx80wb/6PO2VJERORhsndLzZw5E4sXL8Z///tfZGVl4dFHH0VFRQUmTZoEAJgwYQJmzZrlOj8jIwPr16/HqVOnkJWVhbfeegsff/wx/vznP8v1K1yTIAi4satjSviJNr7eTe1uKZGzx4iIyP1k7ZYCgHHjxuHChQuYO3cu8vPzkZKSgrVr17oGGefk5EChqMlgFRUVeOyxx3D27FnodDokJSXhk08+wbhx4+T6FRrlpm6R+H+7z0rjbkbIXY2MnC03ditgKQc0wfLWQ0REfkcQxbb1f5+NRiMMBgNKS0sREhLitesWlpkw8BWp+233nNsRHqj22rVbFVEEXo4BbGbgif1AWEe5KyIiIh/QlL/fsndLtRXRwVokxUqtFG16I01B4IwpIiLyKIYbL3LOmmrzU8I5qJiIiDyI4caLnOvd/HL8AtpYb2BdnA5OREQexHDjRYM6RUAXoMT5UhP25pbIXY589Aw3RETkOQw3XqRTK/H73tIssG/2npe5GhmxW4qIiDyI4cbLRqW0AwB8uz8PVptd5mpk4hpQzJYbIiJyP4YbL7upWxTC9AEoKjdj26mLcpcjDx13BiciIs9huPGyAKUCd/SJA9CGu6bYLUVERB7EcCODUSnxAIC1B/NhqrbJXI0MuM4NERF5EMONDPp3DEN8qA7lZit+PFIodznep+OYGyIi8hyGGxkoFAJGJksDi7/Ze07mamTgHHPDbikiIvIAhhuZOGdNbTxyAaVV1TJX42XObilTKWBvg91yRETkUQw3MkmKDUb3mCBYbHasO5gvdzne5Wy5gQhUlchZCRER+SGGG5kIguAaWPzNvjbWNaUMANTSJqIcd0NERO7GcCOjuxzjbraevIhCo0nmarxMz7VuiIjIMxhuZJQQrkdqxzCIIrB6Xxtb84Zr3RARkYcw3MhsdD+pa2rlzrNta6dwbsFAREQewnAjs7uS20GjUuBoQRn2ny2Vuxzv4RYMRETkIc0KN7m5uTh79qzr5+3bt2PGjBn48MMP3VZYW2HQBWDEdbEAgBU7c2WuxovYLUVERB7SrHDzpz/9CRs3bgQA5Ofn4/bbb8f27dsxe/ZsvPjii24tsC24r38CAOB/e8+jytJG1n3hFgxEROQhzQo3Bw8exMCBAwEAX3zxBa677jps3boVn376KZYtW+bO+tqEGzpHICFchzKzFd8fzJO7HO/gFgxEROQhzQo31dXV0Gg0AIANGzbgrrvuAgAkJSUhL6+N/HF2I4VCwL2pUuvNih1tpGuKWzAQEZGHNCvc9O7dG4sWLcIvv/yC9evXY/jw4QCA8+fPIyIiwq0FthX3pLaHIAC/nS7GmaIKucvxPHZLERGRhzQr3Lz++uv44IMPMHToUNx///1ITk4GAKxevdrVXUVN0y5Uh5u6RQEAVu5qA603rm6pElnLICIi/6NqzoeGDh2KoqIiGI1GhIWFuV5/+OGHodfr3VZcWzOufwJ+PnYBX+46iyfTu0Ol9OOZ+rpQ6ZHdUkRE5GbN+utZVVUFs9nsCjbZ2dmYP38+jh49iujoaLcW2Jak94pGmD4ABUYzfjleJHc5nuXslqquAKxmeWshIiK/0qxwM2rUKHz00UcAgJKSEgwaNAhvvfUWRo8ejffff9+tBbYlGpXStWKx3w8s1hgAwfHPj603RETkRs0KN7t378ZNN90EAPjyyy8RExOD7OxsfPTRR3j33XfdWmBbM26ANGtqQ1YBisr9uEVDoai1SjGngxMRkfs0K9xUVlYiODgYAPDDDz/g7rvvhkKhwA033IDs7Gy3FtjWJMWGoG97A6x2Eav2nJO7HM/iFgxEROQBzQo3Xbt2xapVq5Cbm4t169bh97//PQCgsLAQISEhbi2wLXKuWPzFzlz/3kyTWzAQEZEHNCvczJ07F0899RQSExMxcOBApKWlAZBacfr16+fWAtuikY7NNI8VlGOfP2+myZ3BiYjIA5oVbu655x7k5ORg586dWLdunev12267De+8847bimuram+m+YU/b6ap40J+RETkfs1eSCU2Nhb9+vXD+fPnXTuEDxw4EElJSW4rri1rE5tpcgsGIiLygGaFG7vdjhdffBEGgwEdO3ZEx44dERoaipdeegl2u93dNbZJtTfTXHvIT/fr0nNAMRERuV+zws3s2bOxYMECvPbaa9izZw/27NmDV199Fe+99x7mzJnj7hrbpDaxmSa3YCAiIg9o1vYL//3vf/Hvf//btRs4APTt2xfx8fF47LHH8Morr7itwLZsbGp7vLPhGH49VYzsixXoGBEod0nuxW4pIiLygGa13BQXF9c7tiYpKQnFxfxD5S7xtTbT/HLXWZmr8QDuDE5ERB7QrHCTnJyMBQsWXPH6ggUL0Ldv3xYXRTXu698egBRubHY/W/OG69wQEZEHNKtb6h//+AfuvPNObNiwwbXGzbZt25Cbm4s1a9a4tcC27vZeMQjVByCv1ITNJ4pwS/couUtyn9rr3IgiIAjy1kNERH6hWS03t9xyC44dO4YxY8agpKQEJSUluPvuu3Ho0CF8/PHH7q6xTdOolBidIm2m+dlvfra1hXPMjb0asJTLWwsREfkNQXTj+v779u3D9ddfD5ut9a7LYjQaYTAYUFpa6jNbRRwrKMPv3/kZCgH4+elb0T5ML3dJ7iGKwMsxgM0MPLEfCOsod0VERNRKNeXvd7MX8SPv6R4TjCFdI2AXgY9/9aPWG0HgFgxEROR2DDc+4sHBnQAAy7fn+teKxa5BxRflrYOIiPwGw42P+F1SNBLCdSitqsaqvefkLsd9gqU9tGA8L28dRETkN5o0W+ruu+9u8P2SkpKW1EINUCoETExLxMvfZeG/W8/gjwMSIPjD7CLnOJsSP+puIyIiWTUp3BgMhmu+P2HChBYVRFd3b/8EvPXDMRzJL8Ovp4qR1iVC7pJaLrSD9HiJ4YaIiNyjSeFm6dKlnqqDGsGgC8DY1Hh88msOlm097Sfhhi03RETkXhxz42MmpiUCANYfLkBucaW8xbiDs1uKLTdEROQmDDc+pltMMG7sGgm7CHziD9PCQxOlx/J8oLpK1lKIiMg/MNz4oAcHJwIAPt+eg0qLVd5iWkofDqiDpOelfrg5KBEReR3DjQ+6NSkaHSP0MJqsvr9buCBwUDEREbkVw40PUioETL5RWtTv37+c9v3dwl2Dis/IWgYREfkHhhsfdU9qe4TqA5BTXIl1h/LlLqdlOKiYiIjciOHGR+nVKky4QQoFH/x8Cm7c/9T7OB2ciIjciOHGhz2Qlgi1SoF9uSXYccaHN55kyw0REbkRw40PiwrWYOz17QEAH/58UuZqWsDVcpMjbx1EROQXGG583EM3dYIgABuyCnGisFzucprHOVuqqhgwl8lbCxER+TyGGx/XJSoI6T1jAAD//uWUzNU0kzYE0IVJz9k1RURELcRw4wf+enNnAMBXu8+hsMwkczXNxEHFRETkJgw3fiC1Yxj6dQiFxWbHR1t9NBxwUDEREbkJw40fEATB1Xrz0bYzKDNVy1xRM7DlhoiI3IThxk/8vlcsukQFwmiy4mNf3FCTWzAQEZGbMNz4CYVCwNRbuwIA/vPLaVRZbDJX1ERhidIjp4MTEVELtYpws3DhQiQmJkKr1WLQoEHYvn37Vc9dvHgxbrrpJoSFhSEsLAzp6ekNnt+W3JXcDgnhOlyssGD5Dh8LCbW7pXx5tWUiIpKd7OFmxYoVmDlzJubNm4fdu3cjOTkZw4YNQ2FhYb3nb9q0Cffffz82btyIbdu2ISEhAb///e9x7tw5L1fe+qiUCjxySxcAwIc/n4LFape5oiZwdktZyoHKYnlrISIinyZ7uHn77bcxZcoUTJo0Cb169cKiRYug1+uxZMmSes//9NNP8dhjjyElJQVJSUn497//DbvdjszMTC9X3jrdk9oeMSEa5JWa8NXus3KX03gBWiAoVnrO3cGJiKgFZA03FosFu3btQnp6uus1hUKB9PR0bNu2rVHfUVlZierqaoSHh9f7vtlshtForHP4M41KiSk3STOn3v/pJKw2H2q94XRwIiJyA1nDTVFREWw2G2JiYuq8HhMTg/z8/EZ9xzPPPIN27drVCUi1ZWRkwGAwuI6EhIQW193a/WlQB4QHqpF9sRLfHciTu5zGc3ZNcTo4ERG1gOzdUi3x2muvYfny5fj666+h1WrrPWfWrFkoLS11Hbm5uV6u0vv0ahX+MiQRALBw4wnY7T4yQJcbaBIRkRvIGm4iIyOhVCpRUFBQ5/WCggLExsY2+Nk333wTr732Gn744Qf07dv3qudpNBqEhITUOdqCB9ISEaxR4VhBOdYdalwrmOzYLUVERG4ga7hRq9VITU2tMxjYOTg4LS3tqp/7xz/+gZdeeglr165F//79vVGqzzHoAjDJ0Xozf8Nx32i94SrFRETkBrJ3S82cOROLFy/Gf//7X2RlZeHRRx9FRUUFJk2aBACYMGECZs2a5Tr/9ddfx5w5c7BkyRIkJiYiPz8f+fn5KC8vl+tXaLUm39gZwVoVjhaUYc1BHxh7E1arW8ruQwOhiYioVZE93IwbNw5vvvkm5s6di5SUFOzduxdr1651DTLOyclBXl7NH+b3338fFosF99xzD+Li4lzHm2++Kdev0GoZ9AF46EZp5tT8Dcdha+2tNyHxgKAAbBag3Ee60oiIqNURRLFtLQdrNBphMBhQWlraJsbfGE3VuOn1jSitqsY//5iCUSnxcpfUsHf6AKU5wKS1QMerd00SEVHb0pS/37K33JBnhWgD8LBjx/B/bjje+te9CeOMKSIiahmGmzZg4uBEhOkDcKqoAt/sPS93OQ3joGIiImohhps2IEijcu059e6Px1HdmltvOB2ciIhaiOGmjXggrSMig6RVi1v1nlNcpZiIiFqI4aaN0KtrWm/+ueE4TNU2mSu6inBpfBCKjgFta6w7ERG5CcNNG/LnGzoiPlSH86UmLP75lNzl1C/mOkBQAuUFQGkrbmEiIqJWi+GmDdEGKPHsiCQAwL82nUSB0SRzRfVQ64HY66TnZ3fIWwsREfkkhps25g9945DaMQxV1Ta8vvaI3OXUL96xpca5XfLWQUREPonhpo0RBAFz/9ALAPDV7nPYl1sib0H1aT9AemTLDRERNQPDTRuUnBCKu6+XVip+8dvDaHWLVDvDTd4+wGqRtxYiIvI5DDdt1NPDkqALUGJX9iV8u7+VbaoZ0QXQhgJWE1BwUO5qiIjIxzDctFGxBi0eHSpNDX/t+yOta2q4IADtHeNuzu6UtxYiIvI5DDdt2JSbOqOdQYtzJVX416aTcpdTl7Nr6hzDDRERNQ3DTRumUysx+05pcPGiTSdx6kK5zBXV4pwxxUHFRETURAw3bdwdfWJxS/coWGx2PLfqYOsZXBx/vfRYfAqoLJa3FiIi8ikMN22cIAh4cVRvaFQKbD15sfXsGq4PByK6Sc857oaIiJqA4YbQMSIQ03/XFQDw8neHUVpZLXNFDu3ZNUVERE3HcEMAgIdv7oKu0UEoKrfgH+taycrFznDDQcVERNQEDDcEAFCrFHh5tLSn02fbc7A755LMFaHWSsW7ALtd3lqIiMhnMNyQyw2dIzD2+vYQReDvXx2A1SZzoIjuDah0gLkUuHhC3lqIiMhnMNxQHX+/Iwmh+gAcyS/Df7dly1uMUgW06yc957gbIiJqJIYbqiMiSINnhicBAN7+4SjyS03yFsRBxURE1EQMN3SFcf0TkJIQigqLDS9/d1jeYjiomIiImojhhq6gUAh4efR1UAjAt/vzsPl4kXzFOAcVFxwCLBXy1UFERD6D4YbqdV28ARPSEgEAc785CLNVpo01Q9oBIfGAaAfO75GnBiIi8ikMN3RVM3/fHVHBGpwqqsDin0/JV0jCQOnxxAb5aiAiIp/BcENXFaINwHN39gQAvPfjCeRcrJSnkF6jpMcDX3K9GyIiuiaGG2rQXcntMLhLBMxWO/725T7Y7DJsrNl9OKAJAUpzgZxt3r8+ERH5FIYbapAgCHh1TB/o1Ur8droYi3+RoXsqQAf0vEt6vn+F969PREQ+heGGrikxMhDzRvYCALz1w1EcPFfq/SL63ic9Hl4FWM3evz4REfkMhhtqlPv6J2BY7xhU20Q8sXwPqixenj2VeCMQ3A4wlQLHf/DutYmIyKcw3FCjCIKAjLv7IjpYg5MXKpDxfZZ3C1AogT5jpefsmiIiogYw3FCjhQeq8ca9yQCAj7ZlY+ORQu8W0Hec9HhsHVBV4t1rExGRz2C4oSa5pXsUHhycCAD425f7UGj04t5TMdcBUT0BmwU4/I33rktERD6F4Yaa7NkRSegRE4yicgumf74HVpuX1p4RhJqBxfu/8M41iYjI5zDcUJNpA5RYOP56BDqmh7+z4Zj3Lt7nXukxezNQkuu96xIRkc9guKFm6RodhIyxfQEACzeexMajXhp/E5oAdBwiPT/4pXeuSUREPoXhhprtruR2eOCGjgCAJ1fsxbmSKu9c2Nk1tW85t2MgIqIrMNxQizz3h57oE29ASWU1pn22GxarF8JGr1GAOgi4cAQ49JXnr0dERD6F4YZaRKNS4l/jr0eIVoU9OSWY+81BiKKH95/ShQFDnpCeZ77IFYuJiKgOhhtqsYRwPd6+LwUKAVi+IxcZ3x/xfMBJmwoExQAl2cDOJZ69FhER+RSGG3KL9F4xeO1uaYDxhz+fwsKNJzx7QXUgMHSW9Pynf0jbMhAREYHhhtzovgEJmPMHaYPNN384hmVbTnv2gv0eACK7A1XFwOb5nr0WERH5DIYbcqvJN3bCjPRuAIDn/3cYX+4667mLKVVA+gvS81//BZSe89y1iIjIZzDckNs9cVs3TL6xEwDg6S/3YfW+8567WI8RQIc0wGoCNr3quesQEZHPYLghtxMEAc/d2RP3D0yAXZTWwPluf56nLgbc/pL0fM+nQP5Bz1yHiIh8BsMNeYQgCHhldB/cm9oeNruIx5fvwfcHPBRwEgZIa99ABL6cxMHFRERtHMMNeYxCIeC1sX1x9/XxsNlFTP98D9YezPfMxUa8AYTEA0XHgC8nA3abZ65DREStHsMNeZRSIeCNe5Ixpl88rHYR0z7bjTWeaMEJjgH++Cmg0gEn1gMb5rn/GkRE5BMYbsjjlAoBb96bjFEp7WC1i3js0914Z/0x2O1uXuivXT9g9ELp+db3gL2fu/f7iYjIJzDckFcoFQLeujcZDw5OBAD8M/M4/vrJLpSZqt17oevGAjf/TXr+v8eB3B3u/X4iImr1GG7Ia1RKBZ6/qzf+cU9fqJUKrD9cgDH/2opTF8rde6GhfweS/gDYLMDnfwTO7Xbv9xMRUavGcENed1//BKz46w2ICdHgRGE5Ri3cgq/3nHXfflQKBTDmAyAuGagsApbdCRxd657vJiKiVo/hhmTRr0MY/jf9RqR2DEOZyYonV+zDXz/ehQtlbtrhWxMETPwW6HIbUF0JLL8f2L7YPd9NREStGsMNySY6WIvlD9+Ap37fHQFKAT8cLsDv3/kJ3+5304rG2hDgTyukPahEO7DmKeCH5wC73T3fT0RErZIguq0vwDcYjUYYDAaUlpYiJCRE7nLIISvPiP/7Yh8O5xkBAOk9o/G3YUnoERvc8i8XReCXN4EfX5Z+ju8PpM8DOt3c8u8mIiKvaMrfb4YbajUsVjsWbjyBhRtPwGoXIQjA6JR4PJneHR0i9C2/wP4vgP89IXVTAUDnW4Hb5gLx17f8u4mIyKMYbhrAcNP6nSgsx9vrj2LNAWk1Y5VCwLgBCZiQltjylpyyAqkVZ+dSwO6Yhp70B2DgFCDxZmkwMhERtToMNw1guPEdB86W4o0fjuLnYxdcr/WJN2Ds9fG4KyUe4YHq5n/5pTPAxgxg/woAjv8JhHUCUicCKeOBoOgW1U5ERO7FcNMAhhvf8+upi1iy+TR+PFIIq2NV4wClgFt7ROPu6+Nxa1I0NCpl87688AiwY7HUZWWWxvtAEQB0uRVIuhPoPkLa2oGIiGTFcNMAhhvfVVxhweq95/D/dp/DgXM1O38bdAG4s28cxvSLR2qHMCgUQtO/3FIBHPpa6q46t7PWGwLQ3rHreP9JgDqw5b8IERE1GcNNAxhu/MPR/DJ8tecsvtlzHvlGk+v18EA1buoWiaE9onBTtyhEBmma/uWFR4Aj3wJHvgPO11rdOLidNAC57ziOzSEi8jKGmwYw3PgXm13Eb6cu4qs957DuYD7KzNY67/eJN+DWHlG4NSkafduHQtnUVp3Sc8DRNdJGnCXZ0mtxycCwV4HEG930WxAR0bX4VLhZuHAh3njjDeTn5yM5ORnvvfceBg4cWO+5hw4dwty5c7Fr1y5kZ2fjnXfewYwZM5p0PYYb/1Vts2NPTgk2HS3ET8cu4NB5Y533wwPVuKV7FH7fKwZDe0RDp27COJ1qE7D9A+DnN2vG5vQcCQzLAEIT3PhbEBFRfXwm3KxYsQITJkzAokWLMGjQIMyfPx8rV67E0aNHER195WyVHTt24IsvvkBqaiqefPJJPPPMMww3dFWFRhN+OnYBm45ewM/HLtRp1dEFKDG0RxRG9InD75KiEaRRNe5LK4qATRnS2BzRBqh0wM1PAYOnA6pmdIEREVGj+Ey4GTRoEAYMGIAFCxYAAOx2OxISEjB9+nQ8++yzDX42MTERM2bMYLihRqm22bE7+xIyjxRizYE8nL1U5XpPrVTghi4RuC0pGr9LikZCeCMWDCw4BKz5G5C9Rfo5vAtwxz+Aruke+g2IiNo2nwg3FosFer0eX375JUaPHu16feLEiSgpKcE333zT4OcbG27MZjPM5prNGI1GIxISEhhu2jBRFHHovBFrDuTh+4P5OF1UUef97jFBuLFrFAZ2CsfATuFXX09HFIEDK6X9qsoLpNc63QLcOhvoMMjDvwURUdvSlHDTyLZ49ysqKoLNZkNMTN01RGJiYnDkyBG3XScjIwMvvPCC276PfJ8gCLgu3oDr4g3427AeOHmhAj8eKcCGrELsyr6EYwXlOFZQjiVbTgOQws7ATuFISQhDSkIoOkcGStPNBQHoex/QfTiw6TVg+4fA6Z+ko8ttwK1/B9r3l/m3JSJqe2QLN94ya9YszJw50/Wzs+WGCJCCTtfoIHSNDsLDN3dBSaUFvxwvwm+nL+K3U8U4XljuCjuf/JoDAAjWqpDcPhTXdwhFamI4+nUIRcjwV4FBf5W2dtjzKXAyUzo6DwVSJwE97gBULVhRmYiIGk22cBMZGQmlUomCgoI6rxcUFCA2NtZt19FoNNBoONCTGidUr8bI5HYYmdwOAHCx3IwdZ4qx88wl7M0twYFzpSgzWbH5RBE2nygCIDXg9IgJxvUdwzAw8Vnc0PcxxO59D9j3OXBqk3QERgHJ9wOpDwIRXWT7/YiI2gLZwo1arUZqaioyMzNdY27sdjsyMzMxbdo0ucoiqiMiSIPh18Vh+HVxAKSByUfzy7A3twS7sy9hZ/Yl5BRX4kh+GY7kl+Gz36TWnfjQezC8050YJWYiKX811BUXgK3vSkd8KtB7jLTqcWgHOX89IiK/JPtU8IkTJ+KDDz7AwIEDMX/+fHzxxRc4cuQIYmJiMGHCBMTHxyMjIwOANAj58OHDAIA77rgD48ePx/jx4xEUFISuXbs26pqcLUXuVlhmkoLOmUvYcaYYB88bYbPX/M9KBSt+p9iDSbqfMci2BwrYXe/Z2qVC2fdeIPmPgC5MjvKJiHyCT8yWclqwYIFrEb+UlBS8++67GDRImmkydOhQJCYmYtmyZQCAM2fOoFOnTld8xy233IJNmzY16noMN+RpFWYr9uSUYPuZYhw6V4rDeUbklUpbRESiFMOV23Gn4jcMUmRBIUj/87MIGpyOHQ5b6mR06ntj0xYYJCJqA3wq3Hgbww3J4VKFBVl5RhzOM+LkhQqcLCzHpcJcpJk3437lj+ipyHWdu9feBd/r70Je+xHoGheO7jHB6B4ThIRwPQKU3NOKiNomhpsGMNxQa1JcYcGhcyUoOPQT2p/4HKkVPyEA0krKhWIoPrLejs9st6EYIVAqBCSE6dApMhCJkYGID9Uh1qBFnEGLmBDpYPghIn/FcNMAhhtqzcTyQpRv/Q/Ue5ZAU1UIALAgAP+z34gl1ek4JF7ZLeukUgjoFBmIHrHB6BETjB6xwegWE4wO4fqmbxhKRNTKMNw0gOGGfILVAhz+Bvh1IXB+j+vlsqjrcSDuXvyiHoJcow0FRhPyjSYUlJphsdnr/Sq1UoFOkYHoGh2ELlGB6BARiIQwHRLC9YgJ0TL4EJFPYLhpAMMN+RRRBHK3S6sfH/4GsFdLrwdGASnjgX5/BiK7QRRF5JWacLSgDMfyy3DUMTX9VFE5TNX1hx4ACFAKSAjTo2OEHh0jAtEpMhAdI/RIjAhE+zAdVOzmIqJWguGmAQw35LPKCoDd/5V2JC87X/N6wg1SyOk9BtAE1fmI3S7iXEkVThSW40RhOU4VVeDspUrkFFfi3KUqWO1X/5+/SiEgIVyPxAg9ukYHoVe7EPRuZ0DnyECGHiLyOoabBjDckM+zVQPH1gJ7PgGO/wCIjpaZAD2QdCfQ516gy+8AZUDDX2MXkVdahZyLlThzsRLZFytw5mIFzhRV4szFCpit9bf4aFQK9IiVxvLEh+rQznHEh+qQEK5DsLbh6xIRNQfDTQMYbsivGPOkbR72fAIUn6x5XRfuWAX5LqDD4Cbva2W3i8g3mnCmqAKnL1bgWH4ZDp03IivPiAqLrcHPhgeqkRCuR0KYrs6MrliDDu1CtYgK0kAQOM6HiJqG4aYBDDfkl0QROLcLOLASOPj/gIoLNe+pg6QNPLvdDnRNBwztm30Zu11EdnEljuYbcfZSFc6XmHC+pArnHEdxheWa3xGsUaFzVCC6RAWhc1QgOkUGOaa366FX+/1evkTUTAw3DWC4Ib9nswJnfgYO/D+p26qisO77hg5AwgAgYRDQfgAQ2+eaXViNVWaqRm5xFXIvVSK3uBJ5pSbklVYhr9SE/FITCowmNDDMB7EhWnSM0CM6RIvoYA2igjWICtIgMVKPnnEhDD9EbRjDTQMYbqhNsduB/H3A8Q1S0Dm3s2aMjpNKB7TrB7TvDyQMlAJPcKxHyjFbbci+WIlTF8qllZovlEtdX0UVuFRZ3eBnFQLQJSoI18Ub0DMuGJFBGhh0AQjRBcCgC0CsQYsQjvch8lsMNw1guKE2zVwmdV/lbpeOs9sBU+mV5wXFAnHJjqOv9GhIADw4Vqak0oLTRRXIKa7EhTIzLpSbcaHMjEKjGccKylBYZr7md8QZtOgeE4wkxwKGXaIC0TkyCAY9Qw+Rr2O4aQDDDVEtdjtw8Thwdod05O4ACg8DqOc/C1oDENtX6saKuQ6I6Q1E9QACdF4ptdBowqHzRhw8V4pjheUoqbSgtKradZQ00PITHqiW1vAJ1yMuVIs4gw5xBumRM7yIfAPDTQMYboiuwVwOFBwC8vbVHBeO1CwgWIcAhHcGontKYScuRWrlCWnn0Vae+hhN1dIChs6FDAvKcLqoAgXGa7f4RAap0SFcWrwwIVyPdrUDUKgOQRqO9SGSG8NNAxhuiJrBagGKjgL5B2qOgkNAVXH95wdGSUEn/nogPhVodz0QFOXVkp0qzFacdozryb1UifxSE86XmJBvlGZ7NWaGl0alQKg+AGF6NQy6AEQEqREfqkP7MD0SwqXHDuF6aAOUXviNiNomhpsGMNwQuYkoSlPOCw8DBYelwJO3V2rluXzQMiDN0oq/Xpql1WGQ1MXlpllaLVFmqkb2xUpkX5QWLzx7yTHLq0Sa6WU0WRv1PYIAtA/ToUtUELpEBSExMhChugAEa1WOIwAxIVoYdPL/zkS+iOGmAQw3RB5mqXR0a+0Fzu2WBjAXHcMV43hUOinsdBwMdL5VmqXVxMUGvaHCbEVxhcU1rqekyoILZWacuyRNeT97qQo5xZUoa2QIig3RSju3xwaje0ywa6HD2BAtdGq2/BBdDcNNAxhuiGRgKgXO73UMWnbM0qq6VPecgEAg8UZpwcHOtwBRPQGFb+xhJYoiiissruntJwvLkV1cCWNVNcpMVpSZpceGBj0DQIhWhTiDDu3DpCM+TIf4UD1iDRpEB2sRFaxh1xe1WQw3DWC4IWoF7Hbg4gkgZxtw+mfg1CagsqjuOfpIoNNNQKebgcSbgIiuXh+k7G5GUzWOF0g7th/NL8PxgnIUGE3IN5pQeY1tLZxCtCrEGrSu/bycj/Fh0vOYYA03NiW/xHDTAIYbolbIbgcKDgKnNkpBJ+dXoLqy7jn6SKDDDUCHNOmI7dMqu7GaQxRFlJmtKCg14XypCWcvSbu2n70kbWtRYDShsMwMy1U2M61NqRAQG6JFfKjONe093jn7y/EYpg/g/l7kcxhuGsBwQ+QDrBZprM7pn6Xj7A7AdtmUbqVGmnbevr90xKcCoR19vnXnakRRhLHKisIyk2tbi3OXqnCuxIRzJZU47xgAXW279n/SNSoF4gxaxIRoER6oRqg+AAad9BgVpEFCuDT7KzpYA4XCP+8n+R6GmwYw3BD5IKtZWm8ne6vUqpP765VjdoCahQadqytH9wQiugEBWu/XLAObXZQGOzs2Ms0rkfb1Ol9ShfOlVcgvNaGo/NpT353UKgXiQ3WICFQjVK92TIcPQFigGlFBGkSHSLu8RwVrEBGoZhAij2K4aQDDDZEfEEWg+BRwdqe0X9bZHdIMLVs9f7gFBRCWCEQlAZHdgPAuQEQX6TE41m9beq7GbLWhoNSMvNIqFJSZUVJpkWaBVVajpNKCfKMJuZekliBbQ7ucXiZAKSAmRIs4gxaxjgUQpQCkcQWgUL0aIToVNCoOiqamY7hpAMMNkZ+yWqQ1dvL2Afn7gbz9wIWs+vfOcgrQAyHxUsgJaSc9Brer9XOc9LwVrMfjbVabHXmlJpy9VIVLjgB0ybHlRZFj368LZWYUlZtxscKCpvwl0agUrg1PQ3UBdVqFwgM1iAhSIzJIjQjH84hADafJE8NNQxhuiNoQUQTKC6XQc+GotI/WxZNSq09JDiA2ZoaSAARFS2EnJF46DPFSa1BYJyC8E6AJ9vRv0qpV2+woLDMjv7TKtQBivtHkCkAXys0oNJoavSBifXQBSoQHSqEnJkSLjhF6dIwIlB7DAxEdwmny/o7hpgEMN0QEQGrpKc0FjOeAsnzAeB4oy3M85juOvKvsqXUZfaQj7HSUBjW7HhMBQ/s22fJTH5tdRLnZCqNjs1NjVTVKHIsjSq1DFhRXVDtag8y4WG5BUbm5UYOkASBQrUREkNTaY9AFQKtSQhuggEalhE6tRFSwBnGOafTtDDoGIh/DcNMAhhsiajS7Hai8KAUg4zkp+JSelULRpTNA8emr76/lJCilgBOWCIQm1LT+hMRLrUFBMYAuzGcWLPQ25zT54nILLlZYcLHcjLxSE85crECOY8uM3EtVjZomXx9tgAIGRxeZdKhdg6Zr7ycWog1AiE7leJS21QjgekJexXDTAIYbInIrU6kUckqygUvZ0mNJjhR+SnIAq+na36FQAYHR0uai+khAFyrN/NKGSs91YbWOcOlRHw6oNJ793XyEMwBdLJfCT1G5BUZTNczVNpiq7TBV21BZbUOhURpI7ZxBZm5mIHLSBSgRrFXBoAtAdIgGMcFaRIdoERuiQZA2AEoFoBAEKAQBAUoBUcEaxBp0iA7WMBg1A8NNAxhuiMhr7HagvEAKPMWnpVaf2q1AxnP1T2lvrIBAKeTowwF9hOOIBAIdz52BSBsqPQZGAupAt/16vkwURRhNNV1kzsM1eLrCgkuOGWRlJiuMpppzGrua9NUIAlwzycL00jT7ML00uDpEF4AgjQpBjs1WgzQq18/O58o2OuWe4aYBDDdE1KpYLdLu6hWF0uDnyotSa1BVCWAqkR6rLjmO4prn9e283hgBgVILUWC0NFA6MEo6gqKl8KOPqGkx0oZKg6Xb2HT5a7Ha7Cg3W2tCT2U1CspMKDCakV9qQoHRhAqLDaIowmYXYRdFWKzSoOsCo6nRY4iuxtli5NxtPlgrdZc5X3N2nTm72qTnKujVKgSqVdBrlD7ZctSUv98qL9VERET1Uaml2VeG+MZ/xm4HzKVApSPsVF6sOSqKpH26KotrBSRHILKagOoK4FKF1G3WGIJSau0J0EuPaj2gDnI8DwTUwdKjJrjWESI9akOk7jVNiPRcHQQofH8Ar0qpcExfb/r2H3a7iIsVFuSVVqGo3OwYTC21EF2qtKDcZHUFpzLH8wqzFWVmq2tcUVW1DVXVNhSWma9xtasLUArQq6WWIL1aCb1GhSCNEsEaZyCqCUx6jSMUqZXQq5UIqhWmgjSqVrmXGcMNEZGvUShqupyawlwudZNVXJBaicoLpDBUcaHmqCyuCUQ2izRd3myUDndQqoEAnRSWAvQ144d04Zd1sTlakQIja8YZKX3/T5ZCIY29iQpu+ngpi1VqMSp3tBhJAeiyR8dstNrdaNLMNCuqLDZYbFJAqraJrvdaSqNSIECpQIBScDwq0CfegEUPpLb4u5vL9/+lEBFR42iCpCOiy7XPFUWgukoKOpZKqcXHUiE9t5Q7nlc4npdLwclc5ghCZVLXmtkImIzSc+eUeptFOhpaXPFqtAZH0AmVWoE0IY7fKfjKAde6sJrB2NpQv9hkVa1SIFylRnhg838Xi9WOKosN5RYrqixWVJhtqDBbUWGRHstM1a6xSEbH8yqLDeVmKyod55c7zjNVS0HJbLVfMTg7ziDvlicMN0REdCVBcHRB6Vv+XaLo6BKrknZ7dz6ay+uOJ6osdnSvFTu61hzdbKYS6XtMpdLRnDHYAYGOVqKwWq1EkdJU/KDomkddWE23mh8EosupVQqoVQoY9C1fe8litTuCkRVWm4hqmx3Vjke51w9iuCEiIs8SBEdXlA5AeNM/b7dJ44cqL0ohyFTqaCWqdVw+6NrZvWYyAhCllqfqCsB4tvHXVWocLUMhdccSqTTSwoyKAKmrTBEgdbcpAxzvqaVxSM5Zas4WJOfn1UF+sa6RFJTUCGtBS5KnMNwQEVHrplBK09sDI5r+WbtjzFDVJaCyVgtRVXGtsUeO8UflhVJwqq6QPmszA5VmKVS5leAIOYFSIFKqa8IRILV0QQRESMFQpZVCk0ojPXee6/qsxrEukmMAt7P7zjkDThva5ma8MdwQEZH/UihrxuA0ttHIbpNagyzlUsuP89E5sNpqkcYQ2aodj1bp0WqWXrOZpfFIVZdqpvI7W5Hs1QBE9w7SvhZFgGO6/2VLAGhDHK1OATUtUQqV41BKh1D7USW1OCkdIUulrgleylrhS6WRPUwx3BAREdWmUDq6kkIBgxu/1zn2yFwmBZ3qCikY2cyOgdbOmUsCINT+jFn6nM3ieKyuGZhtswLWqpqB286j8qLUMmV2BKqy89LhLQmDgMk/eO96l2G4ISIi8obaY4+Cor1zzeoqx3T/QqC81mKRFRekFimbVQpJrpYoG2C3SksA2B2H67lVWjzSapYCmTN0WR3PUWtxQoEDiomIiMgTAnTShq2hCZ69jijWdMlZm7+4oLsw3BAREVHLCIJjDI5aGiwtM9+fi0ZERERUC8MNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghIiIiv9Iqws3ChQuRmJgIrVaLQYMGYfv27Q2ev3LlSiQlJUGr1aJPnz5Ys2aNlyolIiKi1k72cLNixQrMnDkT8+bNw+7du5GcnIxhw4ahsLCw3vO3bt2K+++/H5MnT8aePXswevRojB49GgcPHvRy5URERNQaCaIoinIWMGjQIAwYMAALFiwAANjtdiQkJGD69Ol49tlnrzh/3LhxqKiowLfffut67YYbbkBKSgoWLVp0zesZjUYYDAaUlpYiJCTEfb8IEREReUxT/n6rvFRTvSwWC3bt2oVZs2a5XlMoFEhPT8e2bdvq/cy2bdswc+bMOq8NGzYMq1atqvd8s9kMs9ns+rm0tBSAdJOIiIjINzj/bjemTUbWcFNUVASbzYaYmJg6r8fExODIkSP1fiY/P7/e8/Pz8+s9PyMjAy+88MIVryckJDSzaiIiIpJLWVkZDAZDg+fIGm68YdasWXVaeux2O4qLixEREQFBEJr9vUajEQkJCcjNzWX3lofxXnsP77V38X57D++193jqXouiiLKyMrRr1+6a58oabiIjI6FUKlFQUFDn9YKCAsTGxtb7mdjY2Cadr9FooNFo6rwWGhra/KIvExISwv+heAnvtffwXnsX77f38F57jyfu9bVabJxknS2lVquRmpqKzMxM12t2ux2ZmZlIS0ur9zNpaWl1zgeA9evXX/V8IiIialtk75aaOXMmJk6ciP79+2PgwIGYP38+KioqMGnSJADAhAkTEB8fj4yMDADAE088gVtuuQVvvfUW7rzzTixfvhw7d+7Ehx9+KOevQURERK2E7OFm3LhxuHDhAubOnYv8/HykpKRg7dq1rkHDOTk5UChqGpgGDx6Mzz77DM899xz+/ve/o1u3bli1ahWuu+46r9at0Wgwb968K7q8yP14r72H99q7eL+9h/fae1rDvZZ9nRsiIiIid5J9hWIiIiIid2K4ISIiIr/CcENERER+heGGiIiI/ArDTTMtXLgQiYmJ0Gq1GDRoELZv3y53ST4vIyMDAwYMQHBwMKKjozF69GgcPXq0zjkmkwlTp05FREQEgoKCMHbs2CsWdaSmee211yAIAmbMmOF6jffZvc6dO4c///nPiIiIgE6nQ58+fbBz507X+6IoYu7cuYiLi4NOp0N6ejqOHz8uY8W+yWazYc6cOejUqRN0Oh26dOmCl156qc5eRLzXzfPzzz9j5MiRaNeuHQRBuGI/x8bc1+LiYowfPx4hISEIDQ3F5MmTUV5e7pmCRWqy5cuXi2q1WlyyZIl46NAhccqUKWJoaKhYUFAgd2k+bdiwYeLSpUvFgwcPinv37hXvuOMOsUOHDmJ5ebnrnEceeURMSEgQMzMzxZ07d4o33HCDOHjwYBmr9m3bt28XExMTxb59+4pPPPGE63XeZ/cpLi4WO3bsKD744IPib7/9Jp46dUpct26deOLECdc5r732mmgwGMRVq1aJ+/btE++66y6xU6dOYlVVlYyV+55XXnlFjIiIEL/99lvx9OnT4sqVK8WgoCDxn//8p+sc3uvmWbNmjTh79mzxq6++EgGIX3/9dZ33G3Nfhw8fLiYnJ4u//vqr+Msvv4hdu3YV77//fo/Uy3DTDAMHDhSnTp3q+tlms4nt2rUTMzIyZKzK/xQWFooAxJ9++kkURVEsKSkRAwICxJUrV7rOycrKEgGI27Ztk6tMn1VWViZ269ZNXL9+vXjLLbe4wg3vs3s988wz4o033njV9+12uxgbGyu+8cYbrtdKSkpEjUYjfv75594o0W/ceeed4l/+8pc6r919993i+PHjRVHkvXaXy8NNY+7r4cOHRQDijh07XOd8//33oiAI4rlz59xeI7ulmshisWDXrl1IT093vaZQKJCeno5t27bJWJn/KS0tBQCEh4cDAHbt2oXq6uo69z4pKQkdOnTgvW+GqVOn4s4776xzPwHeZ3dbvXo1+vfvj3vvvRfR0dHo168fFi9e7Hr/9OnTyM/Pr3O/DQYDBg0axPvdRIMHD0ZmZiaOHTsGANi3bx82b96MESNGAOC99pTG3Ndt27YhNDQU/fv3d52Tnp4OhUKB3377ze01yb5Csa8pKiqCzWZzraDsFBMTgyNHjshUlf+x2+2YMWMGhgwZ4lp9Oj8/H2q1+oqNT2NiYpCfny9Dlb5r+fLl2L17N3bs2HHFe7zP7nXq1Cm8//77mDlzJv7+979jx44dePzxx6FWqzFx4kTXPa3vvym8303z7LPPwmg0IikpCUqlEjabDa+88grGjx8PALzXHtKY+5qfn4/o6Og676tUKoSHh3vk3jPcUKs0depUHDx4EJs3b5a7FL+Tm5uLJ554AuvXr4dWq5W7HL9nt9vRv39/vPrqqwCAfv364eDBg1i0aBEmTpwoc3X+5YsvvsCnn36Kzz77DL1798bevXsxY8YMtGvXjve6jWG3VBNFRkZCqVReMXOkoKAAsbGxMlXlX6ZNm4Zvv/0WGzduRPv27V2vx8bGwmKxoKSkpM75vPdNs2vXLhQWFuL666+HSqWCSqXCTz/9hHfffRcqlQoxMTG8z24UFxeHXr161XmtZ8+eyMnJAQDXPeV/U1rub3/7G5599ln88Y9/RJ8+ffDAAw/gySefdG28zHvtGY25r7GxsSgsLKzzvtVqRXFxsUfuPcNNE6nVaqSmpiIzM9P1mt1uR2ZmJtLS0mSszPeJoohp06bh66+/xo8//ohOnTrVeT81NRUBAQF17v3Ro0eRk5PDe98Et912Gw4cOIC9e/e6jv79+2P8+PGu57zP7jNkyJArljQ4duwYOnbsCADo1KkTYmNj69xvo9GI3377jfe7iSorK+tstAwASqUSdrsdAO+1pzTmvqalpaGkpAS7du1ynfPjjz/Cbrdj0KBB7i/K7UOU24Dly5eLGo1GXLZsmXj48GHx4YcfFkNDQ8X8/Hy5S/Npjz76qGgwGMRNmzaJeXl5rqOystJ1ziOPPCJ26NBB/PHHH8WdO3eKaWlpYlpamoxV+4fas6VEkffZnbZv3y6qVCrxlVdeEY8fPy5++umnol6vFz/55BPXOa+99poYGhoqfvPNN+L+/fvFUaNGcXpyM0ycOFGMj493TQX/6quvxMjISPHpp592ncN73TxlZWXinj17xD179ogAxLffflvcs2ePmJ2dLYpi4+7r8OHDxX79+om//fabuHnzZrFbt26cCt7avPfee2KHDh1EtVotDhw4UPz111/lLsnnAaj3WLp0qeucqqoq8bHHHhPDwsJEvV4vjhkzRszLy5OvaD9xebjhfXav//3vf+J1110najQaMSkpSfzwww/rvG+328U5c+aIMTExokajEW+77Tbx6NGjMlXru4xGo/jEE0+IHTp0ELVardi5c2dx9uzZotlsdp3De908GzdurPe/zxMnThRFsXH39eLFi+L9998vBgUFiSEhIeKkSZPEsrIyj9QriGKtpRuJiIiIfBzH3BAREZFfYbghIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVxhuiKjNEwQBq1atkrsMInIThhsiktWDDz4IQRCuOIYPHy53aUTko1RyF0BENHz4cCxdurTOaxqNRqZqiMjXseWGiGSn0WgQGxtb5wgLCwMgdRm9//77GDFiBHQ6HTp37owvv/yyzucPHDiA3/3ud9DpdIiIiMDDDz+M8vLyOucsWbIEvXv3hkajQVxcHKZNm1bn/aKiIowZMwZ6vR7dunXD6tWrPftLE5HHMNwQUas3Z84cjB07Fvv27cP48ePxxz/+EVlZWQCAiooKDBs2DGFhYdixYwdWrlyJDRs21Akv77//PqZOnYqHH34YBw4cwOrVq9G1a9c613jhhRdw3333Yf/+/bjjjjswfvx4FBcXe/X3JCI38ch2nEREjTRx4kRRqVSKgYGBdY5XXnlFFEVpt/hHHnmkzmcGDRokPvroo6IoiuKHH34ohoWFieXl5a73v/vuO1GhUIj5+fmiKIpiu3btxNmzZ1+1BgDic8895/q5vLxcBCB+//33bvs9ich7OOaGiGR366234v3336/zWnh4uOt5WlpanffS0tKwd+9eAEBWVhaSk5MRGBjoen/IkCGw2+04evQoBEHA+fPncdtttzVYQ9++fV3PAwMDERISgsLCwub+SkQkI4YbIpJdYGDgFd1E7qLT6Rp1XkBAQJ2fBUGA3W73RElE5GEcc0NErd6vv/56xc89e/YEAPTs2RP79u1DRUWF6/0tW7ZAoVCgR48eCA4ORmJiIjIzM71aMxHJhy03RCQ7s9mM/Pz8Oq+pVCpERkYCAFauXIn+/fvjxhtvxKeffort27fjP//5DwBg/PjxmDdvHiZOnIjnn38eFy5cwPTp0/HAAw8gJiYGAPD888/jkUceQXR0NEaMGIGysjJs2bIF06dP9+4vSkRewXBDRLJbu3Yt4uLi6rzWo0cPHDlyBIA0k2n58uV47LHHEBcXh88//xy9evUCAOj1eqxbtw5PPPEEBgwYAL1ej7Fjx+Ltt992fdfEiRNhMpnwzjvv4KmnnkJkZCTuuece7/2CRORVgiiKotxFEBFdjSAI+PrrrzF69Gi5SyEiH8ExN0RERORXGG6IiIjIr3DMDRG1auw5J6KmYssNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+ZX/D9iPigAKZRmtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n",
            "Trained model saved to 'trained_model_oligodendroyctes.pth'\n",
            "Test Loss: 0.03943661227822304\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03943661227822304"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Include a step to save the csv file for the test dataset\n",
        "df = pd.read_csv('finaldataset_oligodendrocytes.csv')\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "        self.weight_tensor = None #weight\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Specify additional features for each cell\n",
        "        additional_features = ['age_group', 'sex']\n",
        "\n",
        "        # Use DataFrame to get additional categorical features\n",
        "        additional_categorical_features = list(set(additional_features))\n",
        "\n",
        "        # Fill missing values in specific columns with a default value (e.g., 0)\n",
        "        #default_value = 0\n",
        "        #self.df.fillna(default_value, inplace=True)\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Identify gene start columns\n",
        "        start_columns = self.df.filter(like='_start_position').columns\n",
        "\n",
        "        # Identify gene stop columns\n",
        "        stop_columns = self.df.filter(like='_end_position').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = (chromatin_accessibility_columns.union(additional_categorical_features).union(start_columns).union(stop_columns)\n",
        ")\n",
        "        # Select features and target features\n",
        "        X = self.df[feature_columns].copy()\n",
        "        y = self.df.filter(like='_Expression').copy()\n",
        "\n",
        "        # Convert categorical columns to one-hot encoding\n",
        "        X = pd.get_dummies(X, columns=additional_categorical_features)\n",
        "\n",
        "        # Train-validation-test split\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Reshape input data for CNN\n",
        "        self.X_train_tensor = torch.tensor(X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "        # Give less weight to missing values\n",
        "        missing_mask_train = X_train.isnull().astype(int)\n",
        "        weight_tensor = torch.ones_like(self.y_train_tensor)  # Initialize with ones\n",
        "        weight_tensor[missing_mask_train.values] = 0.5  # Assign lower weights to missing values\n",
        "        self.weight_tensor = weight_tensor\n",
        "\n",
        "        self.X_val_tensor = torch.tensor(X_val.values.reshape(X_val.shape[0], X_val.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "        # Concatenate DataFrames\n",
        "        test_dataset = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "        # Save the test dataset to a CSV file\n",
        "        test_dataset.to_csv(\"final_test.csv\", index=False)\n",
        "\n",
        "        print(f'Test dataset saved to \"final_test.csv\"')\n",
        "\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "gene_predictor.preprocess_data()"
      ],
      "metadata": {
        "id": "aMol2DsHg3DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMTIYAZ VERSION 5 - Testing code on the test dataset using the trained model\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#comment out if it is already loaded\n",
        "df = pd.read_csv('final_test.csv')\n",
        "\n",
        "\n",
        "class PairedCNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.2, num_genes=652):\n",
        "        super(PairedCNNModel, self).__init__()\n",
        "\n",
        "        self.input_size = input_size  # Set input size during initialization\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(31296, hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Embedding layer for gene identity\n",
        "        self.embedding = nn.Embedding(num_genes, hidden_size)\n",
        "\n",
        "        # Linear layers for gene-specific information\n",
        "        self.fc_gene = nn.Linear(1, hidden_size)\n",
        "        self.batch_norm_gene = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout_gene = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size + 1, 652)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # Process gene-specific information\n",
        "        a = a.squeeze(dim=-1)\n",
        "        a_embedding = self.embedding(a.long())\n",
        "        a_embedding = a_embedding.squeeze(dim=1)\n",
        "        a_embedding = F.relu(a_embedding)\n",
        "\n",
        "        # Concatenate shared and gene-specific information\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "\n",
        "        # Final linear layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class GeneExpressionPredictor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.X_train_tensor = None\n",
        "        self.y_train_tensor = None\n",
        "        self.X_val_tensor = None\n",
        "        self.y_val_tensor = None\n",
        "        self.X_test_tensor = None\n",
        "        self.y_test_tensor = None\n",
        "\n",
        "        self.weight_tensor = None #weight\n",
        "\n",
        "    def preprocess_data(self):\n",
        "\n",
        "        # Specify additional features for each cell\n",
        "        # Identify columns that contain the words \"age_group\" or \"sex\"\n",
        "        selected_feature_columns = self.df.filter(like='age_group').columns.union(self.df.filter(like='sex').columns)\n",
        "\n",
        "        # Use the selected columns as additional features\n",
        "        additional_categorical_features = list(set(selected_feature_columns))\n",
        "\n",
        "        # Identify chromatin accessibility columns\n",
        "        chromatin_accessibility_columns = self.df.filter(like='_ChromatinAccessibility').columns\n",
        "\n",
        "        # Identify gene start columns\n",
        "        start_columns = self.df.filter(like='_start_position').columns\n",
        "\n",
        "        # Identify gene stop columns\n",
        "        stop_columns = self.df.filter(like='_end_position').columns\n",
        "\n",
        "        # Combine all feature names (numeric, categorical, and additional features)\n",
        "        feature_columns = (chromatin_accessibility_columns.union(additional_categorical_features).union(start_columns).union(stop_columns)\n",
        ")\n",
        "\n",
        "        # Select features and target features\n",
        "        X_test = self.df[feature_columns].copy()\n",
        "        y_test = self.df.filter(like='_Expression').copy()\n",
        "\n",
        "        # Reshape data\n",
        "        self.X_test_tensor = torch.tensor(X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
        "        self.y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "    def train_model(self, hidden_size=128, dropout_rate=0.2, epochs=100):\n",
        "        print(\"Welcome. Using scGenePredix to train the model.\")\n",
        "        input_size = self.X_train_tensor.shape[1]\n",
        "\n",
        "        model = PairedCNNModel(input_size=input_size, hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Store the training and validation loss for plotting\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        print(\"Training in progress...\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(self.X_train_tensor, self.X_train_tensor[:, -1:, -1:])\n",
        "            #loss = criterion(outputs, self.y_train_tensor)\n",
        "            loss = torch.mean(self.weight_tensor * (outputs - self.y_train_tensor) ** 2) #loss with weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(self.X_val_tensor, self.X_val_tensor[:, -1:, -1:])\n",
        "                val_loss = criterion(val_outputs, self.y_val_tensor)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "              print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "        # Plot the training and validation loss\n",
        "        plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.ylim(0, 0.5)  # Adjust y-axis limits\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "        torch.save(model.state_dict(), 'trained_model_oligodendroyctes.pth')\n",
        "        print(\"Trained model saved to 'trained_model_oligodendroyctes.pth'\")\n",
        "        self.model = model\n",
        "\n",
        "    def load_model(self, model_path='trained_model_oligodendroyctes.pth'):\n",
        "        # Pass the input_size to the model during initialization\n",
        "        input_size = 31296\n",
        "        self.model = PairedCNNModel(input_size=input_size, hidden_size=128, dropout_rate=0.2)\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        self.model.eval()\n",
        "        print(f\"Model loaded from {model_path}\")\n",
        "\n",
        "    def evaluate_model(self):\n",
        "\n",
        "        model = self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(self.X_test_tensor, self.X_test_tensor[:, -1:, :])\n",
        "            test_loss = criterion(test_outputs, self.y_test_tensor)\n",
        "\n",
        "        print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "        return test_loss.item()\n",
        "\n",
        "\n",
        "# Create an instance of GeneExpressionPredictor\n",
        "gene_predictor = GeneExpressionPredictor(df)\n",
        "\n",
        "# Preprocess the data\n",
        "gene_predictor.preprocess_data()\n",
        "\n",
        "# Train the model\n",
        "#gene_predictor.train_model()\n",
        "\n",
        "# Load the model\n",
        "gene_predictor.load_model()\n",
        "\n",
        "# Evaluate the model\n",
        "gene_predictor.evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odHqOTfDay0c",
        "outputId": "8c15287a-aca6-42ed-af0b-f2e9734d2116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from trained_model_oligodendroyctes.pth\n",
            "Test Loss: 0.03943661227822304\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03943661227822304"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}